						Metacomputing
1992,515cite
构建元计算机的第一阶段主要是软件和硬件的集成工作，它包括将所有资源与高性能网络互连，实现分布式文件系统，协调跨系统的用户访问
元计算机发展的下一个阶段超越了异构计算机网络的软件集成。第二阶段涉及将单个应用程序扩展到多台计算机上，允许中心的异构计算机集合协同工作以解决单个问题。这使用户能够尝试没有元计算机几乎不可能实现的各种计算。允许以一般方式完成此操作的软件(与一次性、临时解决方案相反)刚刚出现，并且随着用户开始使用它，正在进行评估和改进。
元计算机发展的第三阶段将是一个透明的国家网络，它将极大地增加应用程序可用的计算和信息资源。这个阶段涉及的不仅仅是让本地元计算机使用远程资源(即，改变组件之间的距离)。第三阶段包括建立适当的WAN基础设施和开发管理、文件系统和安全方面的标准。
作者以NCSA的元计算机作为第一阶段的范例，以SIGGRAPH在92年的文献作为第二阶段开始的范例，在理论模拟，分子虚拟现实等方面展示了元计算的部分成果
						The Grid: A New Infrastructure for 21st Century Science
1998，5774cite
衡量技术变革速度的一个有用指标是，速度或容量翻番或价格减半的平均时间。对于存储、网络和计算能力，这些周期分别为12个月、9个月和18个月左右。
在这篇文章中，作者认为计算机速度每18个月翻一番的速度无法跟上增长更快的存储和传输，探讨了"网格计算"这一概念
作者提到，身份验证、授权和策略是网格中最具挑战性的问题。传统的安全技术主要关注保护客户机和服务器之间的交互，而在网格环境中，情况更为复杂，客户机和服务器之间的区别趋于消失
单点登录(用户应该能够进行一次身份验证，然后为计算分配操作的权利)，映射到本地安全机制(不同的站点可能使用不同的本地安全解决方案,网格安全基础设施需要映射到每个站点上的这些本地解决方案)，代理凭证(必须仔细管理这些委托操作和启用它们的授权证书)，社区授权和策略(每个资源都要跟踪社区成员和特权是不可行的,需要能够根据其他标准来表达策略)
						A Resource Management Architecture for Metacomputing Systems
1998,929cite
元计算系统旨在支持远程和/或并发使用地理上分布的计算资源。元计算系统允许应用程序根据需要组装和使用计算资源集合，而不考虑物理位置。
元计算环境引入了五个具有挑战性的资源管理问题:站点自治、异构基础、策略可扩展性、共同分配和在线控制
作者描述了一个解决这些问题的资源管理体系结构。该体系结构将资源管理问题分布在不同的本地管理器、资源代理和资源共同分配器组件之间，并定义了一种可扩展的资源规范语言来交换有关需求的信息。

站点自治问题指的是资源通常由不同的组织在不同的管理域中拥有和操作；
异构基板问题源于站点自治问题，是指不同站点可能使用不同的本地资源管理系统；
策略可扩展性问题的出现是因为元计算应用程序来自广泛的领域，每个领域都有自己的需求；
共同分配问题是因为许多应用程序的资源需求只能通过在几个站点同时使用资源来满足。站点自治和在分配过程中出现故障的可能性导致需要专门的机制来分配多个资源；
在线控制问题之所以出现，是因为可能需要进行大量协商，以使应用程序需求适应资源可用性，特别是在执行过程中需求和资源特征发生变化时；

先前关于元计算系统资源管理的工作可以分为两大类:
网络批处理排队系统。这些系统严格关注一组联网计算机的资源管理问题。批调度系统提供了一种有限形式的策略可扩展性
广域调度系统。在这里，资源管理作为将应用程序组件映射到资源并调度其执行的组件来执行。到目前为止，这些系统没有解决异构底物、场地自治和共同分配的问题。

文章定义了一种资源规范语言(RSL)作为组件之间通信对资源的请求,并构造了一个多层的资源体系结构(从应用程序到资源代理，到资源共同分配器和资源管理器)
资源代理负责获取高级RSL规范，并通过作者称为专门化的过程将其转换为更具体的规范
这样的地面请求可以传递给共同分配器，该分配器负责协调多个站点的资源分配和管理
资源共同分配器将多请求分解为其组成元素，并将每个组件传递给适当的资源管理器
   						The Harness Metacomputing Framework
1999,45cite
Harness是一个实验性的元计算框架，它基于动态可重构的原则，不仅针对组成虚拟机的计算机和网络，而且还针对虚拟机本身的功能。Harness的这个基本特性旨在解决当前元计算框架的不灵活性，以及它们无法合并新技术和避免快速过时的问题
分布式和集群计算技术经常随着新的机器功能、互连网络类型、协议和应用程序需求而变化，底层中间件要么需要更改，要么需要重建，从而增加了所涉及的工作量，并阻碍了互操作性。

这个框架支持重新配置组成虚拟机的计算机和网络，还支持重新配置虚拟机本身的功能。这些特征可以通过作为系统中心特征的“插件”机制在用户控制下进行修改，提供一个可以动态适应以满足应用程序需求的虚拟机环境
Harness元计算框架中的基本抽象是分布式虚拟机(DVM)，用户可以通过插件的方式配置，加入或离开DVM


相关工作：
	PVM是最早用具体的虚拟机和编程环境术语提出元计算概念，并探索异构网络计算的系统之一。PVM基于动态的、用户指定的主机池的概念，软件在此基础上模拟通用的并发计算资源。
动态进程管理与PVM中的强类型异构消息传递相结合，为分布式内存并行程序提供了一个有效的环境。然而，PVM在许多方面缺乏灵活性，这可能会限制下一代元计算和协作应用程序的发展。例如，不支持多个DVM合并和分裂。两个不同的用户不能在一个活动的PVM机器中交互、协作和共享资源和程序。PVM使用互联网协议，这可能会妨碍使用专门的网络硬件
	Legion是一个元计算系统，可以容纳地理上分布的高性能机器和工作站的异构组合。Legion是一个面向对象的系统，其重点是提供对企业级分布式计算框架的透明访问。因此，它并不试图迎合不断变化的需求，并且在它所支持的计算模型类型和实现中都是相对静态的。
	Globus是建立在“Nexus”通信框架之上的元计算基础设施。Globus系统是围绕工具包的概念设计的，该工具包由与通信、资源分配、数据等相关的预定义模块组成。然而，这些模块的组装不应该像在Harness中那样在运行时动态地发生。
几乎所有上述项目都设想了一个模型，在这个模型中，非常高性能的模块被静态地连接起来，以构建一个更大的系统。Harness项目的主要思想之一是通过动态连接、断开连接和重新配置异构组件来交换一些效率，以获得增强的全局可用性、可升级性和故障恢复能力。


todo：自动切割程序分割到分布式平台

						MapReduce: Simplified Data Processing on Large Clusters
2004，14048cite；2008，22950cite	

这是谷歌的一项工作，MapReduce 是一种编程模型和关联的实现，用于处理和生成大规模数据集。用户定义 Map 函数来处理输入数据，并生成一组中间键值对，然后定义 Reduce 函数来处理这些中间键值对。MapReduce 自动处理数据分割、分发和故障恢复，使得程序可以在分布式系统中高效运行。	

作者设计了一个新的抽象，它允许用户表达试图执行的简单计算，但隐藏了库中并行化、容错、数据分布和负载平衡的混乱细节。
作者解释道，大多数计算涉及到对输入中的每个逻辑记录应用map操作，以计算一组中间键/值对，然后对共享同一键的所有值应用reduce操作，以便适当地组合派生数据。使用带有用户指定map和reduce操作的功能模型，能够轻松地并行化大型计算，并使用重执行作为容错的主要机制。
这项工作的主要贡献是一个简单而强大的接口，它可以实现大规模计算的自动并行化和分布，并结合该接口的实现，在大型商用pc集群上实现高性能。该编程模型还可以用于在同一台机器的多个核心之间并行化计算。

用户程序中的MapReduce库首先将输入文件分成M个块，每个块通常为16-64MB(由用户通过可选参数控制)。然后，它在一组机器上启动该程序的许多副本。
 该程序的一个副本master是特别的。其余的是由master分配工作。有M个map任务和R个reduce任务要分配。
被分配映射任务的工作线程读取相应输入分割的内容。它从输入数据中解析键值对，并将每对传递给用户定义的map函数，map函数产生的中间键值对在内存中进行缓冲。
周期性地将缓冲对写入本地磁盘，并通过分区函数划分为R个区域。这些缓冲对在本地磁盘上的位置被传递回主服务器，主服务器负责将这些位置转发给reduce worker
当主服务器通知reduce worker有关这些位置时，它使用远程过程调用从map worker的本地磁盘读取缓冲数据。当reduce worker读取了其分区的所有中间数据后，它按中间键对数据进行排序，以便将出现的所有相同键分组在一起
reduce worker遍历已排序的中间数据，对于遇到的每个唯一的中间键，它将键和相应的中间值集传递给用户的reduce函数。reduce函数的输出被附加到这个reduce分区的最终输出文件中
主程序唤醒用户程序。此时，用户程序中的MapReduce调用返回到用户代码

拓展功能
用户指定的分区函数，用于确定中间键值到R reduce分片的映射
排序保证:我们的实现保证在每个R reduce分区中，中间键/值对以递增的键顺序处理
用户指定的组合函数，用于在相同的map任务中使用相同的键对生成的中间值进行部分组合(以减少必须通过网络传输的中间数据量)
自定义输入和输出类型，用于读取新的输入格式和产生新的输出格式
在一台机器上执行的模式，以简化调试和小规模测试。

						Apache Hadoop YARN: Yet Another Resource Negotiator
2013，2844cite
Apache Hadoop最初是MapReduce的众多开源实现之一。开发人员为了充分利用物理资源，经常采用巧妙的变通方法来避开MapReduce API的限制。
这些限制和误用激发了一整类将Hadoop作为不相关环境基准的论文。到目前为止，学术界和开源社区都很好地理解了原始Hadoop架构的局限性

作者介绍了下一代Hadoop计算平台YARN，与熟悉的单片架构不同，通过将资源管理功能与编程模型分离，YARN将许多与调度相关的功能委托给每个作业组件。在这个新的上下文中，MapReduce只是运行在YARN之上的应用程序之一。这种分离为选择编程框架提供了很大的灵活性

每个集群的ResourceManager (RM)跟踪资源使用情况和节点活跃度，执行分配不变量，并仲裁租户之间的争用。作为专用机器上的守护进程运行，作为集群中各种竞争应用程序之间的资源仲裁中心。它可以跨租户强制执行属性，如公平性、容量和局部性。根据应用程序需求、调度优先级和资源可用性，RM动态地将租约(称为容器)分配给在特定节点上
ApplicationMaster (AM)通过从RM请求资源、从它接收到的资源生成物理计划以及围绕故障协调该计划的执行来协调单个作业的逻辑计划。
为了执行和跟踪这些分配，RM与运行在每个节点上的称为NodeManager (NM)的特殊系统守护进程进行交互。RM和NMs之间的通信是基于心跳的，以实现可伸缩性。NMs负责监控资源可用性、报告故障和容器生命周期管理。

作业通过一个公共提交协议提交给RM，并经过一个准入控制阶段，在此阶段，安全凭证被验证，各种操作和管理检查被执行。接受的作业被传递给要运行的调度程序。一旦调度器拥有足够的资源，应用程序就会从接受状态转到运行状态。为AM分配一个容器，并在集群中的一个节点上生成它。接受的应用程序的记录被写入持久存储，并在RM重新启动或出现故障时恢复。
ApplicationMaster是作业的“头”，管理所有生命周期方面，包括动态增加和减少资源消耗，管理执行流(例如，根据映射的输出运行reducer)，处理故障和计算倾斜，以及执行其他局部优化。实际上，AM可以运行任意用户代码，并且可以用任何编程语言编写，因为与RM和NM的所有通信都是使用可扩展通信协议进行编码的
为了获取容器，AM向RM发出资源请求。当代表AM分配资源时，RM为该资源生成一个租约，该租约由后续的AM心跳提取。当AM向NM提交容器租约时，基于令牌的安全机制保证其真实性
