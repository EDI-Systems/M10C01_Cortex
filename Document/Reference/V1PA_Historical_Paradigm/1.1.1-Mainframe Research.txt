						wiki百科-mainframe computer
大型计算机是大型组织主要用于关键应用程序的计算机。
大型计算机很大，但不如超级计算机大，并且比其他一些类别的计算机具有更高的处理能力。
大多数大型计算机系统架构是在 1960 年代建立的，但它们仍在继续发展。大型计算机通常用作服务器。
“大型机”一词源自大型机柜，称为主机，用于容纳早期计算机的中央处理器和主存储器。
后来，大型机一词被用来区分高端商用计算机和功能较弱的机器。

现代大型机设计的特点较少是原始计算速度，而更多的是：
冗余内部工程，实现高可靠性和安全性
广泛IO设施，能够卸载到单独的引擎
与旧软件严格向后兼容
通过虚拟化实现高硬件和计算利用率，支持海量吞吐量
硬件热插拔，如处理器和内存
大型机的高稳定性和可靠性使这些机器能够在很长一段时间内不间断地运行，平均故障间隔时间以数十年为单位。

大型机的特点
现代大型机可以同时运行多个不同的操作系统实例。这种虚拟机技术允许应用程序像在物理上不同的计算机上一样运行。
大型机旨在处理非常大的输入和输出 （I/O），并强调吞吐量计算。
自 1950 年代后期以来，大型机设计包括管理 I/O 设备的附属硬件（称为通道或外围处理器），使 CPU 只能自由处理高速内存。
大型机还具有用于容错计算的执行完整性特性。
例如，z900、z990、System z9 和 System z10 服务器可以有效地执行两次面向结果的指令，比较结果，在任何差异之间进行仲裁（通过指令重试和故障隔离），
然后将“动态”工作负载转移到正常运行的处理器（包括备件），而不会对操作系统、应用程序或用户产生任何影响。
这种硬件级功能，也出现在惠普的NonStop系统中，被称为锁步进，因为两个处理器一起执行它们的“步骤”（即指令）。
并非所有应用程序都绝对需要这些系统提供的有保证的完整性，但许多应用程序都需要，例如金融交易处理。

与超级计算机的区别
超级计算机是处于数据处理能力前沿的计算机，就计算速度而言。超级计算机用于处理数字和数据的科学和工程问题（高性能计算），而大型机则专注于事务处理。
事务可以指一组操作，包括磁盘读/写、操作系统调用或从一个子系统到另一个子系统的某种形式的数据传输，这些操作不是由 CPU 的处理速度来衡量的。
大型机和超级计算机并不总是能清楚地区分开来;直到 1990 年代初，许多超级计算机都基于具有超级计算扩展的大型机架构


Mainframe computers are the computers that large organizations use primarily for critical applications.
Mainframe computers are large, but not as large as supercomputers, and have higher processing power than some other classes of computers.
Most mainframe computer system architectures were built in the 1960s, but they continue to evolve. Mainframe computers are often used as servers.
The term "mainframe" comes from the large cabinet, called mainframe, used to house the central processing unit and main memory of early computers.
Later, the term mainframe was used to distinguish high-end commercial computers from less powerful machines.
​
Modern mainframe designs are characterized less by raw computing speed and more by:
Redundant internal engineering to achieve high reliability and security
Extensive IO facilities, capable of offloading to separate engines
Strict backward compatibility with legacy software
Enabling high hardware and compute utilization through virtualization to support massive throughput
Hardware hotplugging, such as processors and memory
The high stability and reliability of mainframes enables these machines to operate uninterrupted for long periods of time, with mean time between failures measured in decades.

Characteristics of mainframes
Modern mainframes can run multiple different instances of the operating system simultaneously. This virtual machine technology allows applications to run as if they were on physically different computers.
Mainframes are designed to handle very large inputs and outputs (I/O) with an emphasis on throughput calculations.
Since the late 1950s, mainframe designs have included accessory hardware (called channels or peripheral processors) to manage I/O devices, leaving the CPU only free to handle high-speed memory.
Mainframes also have execution integrity features for fault-tolerant computation.
For example, z900, z990, System z9, and System z10 servers can efficiently execute result-oriented instructions twice, compare the results, arbitrate between any differences (via instruction retry and fault isolation),
The "dynamic" workload is then offloaded to a functioning processor, including spare parts, without any impact on the operating system, applications, or users.
This hardware-level capability, also present in HP's NonStop system, is called lock-stepping because the two processors execute their "steps" -that is, instructions -together.
Not all applications absolutely require the guaranteed integrity provided by these systems, but many do, such as financial transaction processing.
​
The difference from a supercomputer
Supercomputers are computers at the cutting edge of data processing power, in terms of computational speed. Supercomputers are used to handle scientific and engineering problems with numbers and data (high performance computing), while mainframes focus on transaction processing.
A transaction can refer to a set of operations, including disk reads/writes, operating system calls, or some form of data transfer from one subsystem to another, that are not measured by the processing speed of the CPU.
Mainframes and supercomputers are not always clearly separated; Until the early 1990s, many supercomputers were based on mainframe architectures with supercomputing extensions



						The mainframe is dead. Long live the mainframe!
2013
这篇文章涉及的技术细节不多，很大篇幅讨论了工业界和学术界的大型机课程和人才资源，但也介绍了大型机的业界情况

大型企业计算包括两个主要的计算体系结构。
第一种使用一个或几个非常大的服务器，为数百或数千个客户机提供批处理和在线事务处理。这些服务器在历史上被称为大型机
第二种也是最新的体系结构是将许多商品服务器并行集群，并让它们以分布式方式处理数百或数千个客户机的事务。

在过去的几年中，由于经济、可扩展性和故障转移能力的原因，集群被吹捧为大型机的合乎逻辑的继承者，IBM的大型机硬件和软件的销量不断增加。

作者认为，具有大型机知识、大型企业语言、数据库和系统知识以及集成知识的人员需求将会更大，
解决方法之一是研究大型机和客户/服务器技术的集成，而不是在大型机以外的完全不同的计算机平台上用新语言重新设计和重写遗留代码。
较新的大型机技术(如IBM的WebSphere)使大型机适应客户机/服务器和web环境，因此没有必要放弃大型机环境，因为大型机环境提供了一个稳定和安全的平台，允许组织实现其目标。

This article is short on technical details.It discusses the mainframe curriculum and talent resources in industry and academia at large, but it also gives an overview of the mainframe industry
​
Large enterprise computing includes two main computing architectures.
The first uses one or a few very large servers that provide batch processing and online transaction processing to hundreds or thousands of clients. These servers were historically referred to as mainframes
The second and most recent architecture is to cluster many commodity servers in parallel and have them process transactions for hundreds or thousands of clients in a distributed fashion.
​
Over the past few years, clusters have been touted as the logical successor to the mainframe for reasons of economics, scalability, and failover capabilities, and IBM has seen increasing sales of mainframe hardware and software.
​
The authors believe that there will be greater demand for people with knowledge of mainframes, large enterprise languages, databases and systems, and integration knowledge.
One solution is to investigate the integration of mainframe and client/server technologies, rather than redesign and rewrite legacy code in a new language on a completely different computer platform outside the mainframe.
Newer mainframe technologies, such as IBM's WebSphere, adapt the mainframe to client/server and web environments, so there is no need to abandon the mainframe environment, which provides a stable and secure platform that allows organizations to achieve their goals.
						Using Hadoop on the Mainframe:A Big Solution for the Challenges of Big Data 

2015，6cite
在云计算和“大数据”时代，作者说明了说明大型机仍然是一个有价值的架构。作者认为，大型机在处理大量数据方面提供了其他解决方案无法比拟的优势

作者首先讨论了数据的框架，据业界估计，世界上20%的数据是“结构化的”，或者以某种表格形式存在，但80%的数据是“半结构化的”——社交媒体、呼叫中心数据、文本文件、传感器数据等。
这不仅在数量和速度上提出了挑战，而且必须使用某种方法从大量数据中收集一些含义，而这些数据与提供数据含义指示的正式模式无关。这最终导致了对改进的分析的需求。	

第一个挑战是在概念上对大数据进行分类，使这项任务更易于管理。第一个合乎逻辑的步骤是将“敏感的、结构化的”数据与“半公开的、半结构化的”数据分开。
	第一类包括客户信息、业务事务等，它们通常驻留在关系表中。
	后一类包括呼叫中心数据，声明数据、文本文件、XML格式的数据、社交媒体数据等等。
	采用多种格式的数据所面临的挑战(如上所述的多种格式)是，必须从数据所在的位置提取数据，将其转换为所需的格式，并将其加载到需要使用它的位置。

ETL问题
	从不同的系统中提取、转换和加载(ETL)数据似乎无关紧要。但是，必须考虑到ETL的相当大的成本。
	1)额外的服务器、存储和网络设备需要支持文件传输和处理数据
	2)管理文件传输及相关数据所涉及的工时
	3)与浪费的系统周期(系统误用)有关的卸载成本


为什么在大型机上使用hadoop?
	通过实现开源解决方案hadoop，可以将大型机软件的许可成本降至最低。这可能影响业务的支持和维护成本。
	将工作流现代化到大数据开源环境，扩大了未来工作流更新的范围，容易构建了适应新技术的系统，从而拓宽了开发方法
	
	大型机能够创建虚拟Linux服务器。这些服务器可以运行大部分可用的开源产品，能够将运行hadoop的Linux集群放在同一台主机上，该主机包含各种专有格式的大型数据存储。
	可以相对轻松地创建、移动和删除它们，并且几乎不会破坏基础设施的物理足迹。因此，如果要开发可以在内部执行ETL的流程(即应用程序)，则可以消除大型机到非大型机ETL的许多复杂性和成本。
	大型机的“按需使用”特性允许在对现有基础设施的干扰最小的情况下进行扩展和缩小，并且对基础设施的物理足迹的影响最小。例如们可以通过发出几个命令使可用集群增加一倍或三倍，而不会中断网络

作者认为，在大型机上使用hadoop来分析数据，可以显著降低分析计算的成本，并通过避免在源(大型机)平台之外创建不必要的数据副本，大大提高遵从性

In the era of cloud computing and "big data," the authors illustrate that the mainframe is still a valuable architecture. The authors argue that mainframes offer advantages over other solutions in processing large amounts of data
​
The authors first discuss the framing of data, industry estimates that 20% of the world's data is "structured", or exists in some form of tabular form, but 80% of the data is "semi-structured" - social media, call centre data, text files, sensor data, etc.
Not only does this present a challenge in volume and speed, but some method must be used to gather some meaning from a large amount of data that is independent of the formal schema that provides an indication of the meaning of the data. This ultimately leads to the need for improved analysis.
​
The first challenge is to conceptually classify big data to make this task more manageable. The first logical step is to separate "sensitive, structured" data from "semi-public, semi-structured" data.
The first category includes customer information, business transactions, etc., which typically reside in relational tables.
The latter category includes call center data, declaration data, text files, data in XML format, social media data, and so on.
The challenge with data in multiple formats (as mentioned above with multiple formats) is that you have to take the data from where it is, convert it to the format you want, and load it to where you want to use it.
​
ETL Issues
Extracting, Transforming, and loading (ETL) data from different systems may seem irrelevant. However, the considerable cost of ETL must be taken into account.
1) Additional servers, storage, and network equipment needed to support file transfer and process data
2) Manage the man-hours involved in file transfer and related data
3) Offloading costs associated with wasted system cycles (system misuse)
​
​
Why use hadoop on mainframes?
Licensing costs for mainframe software can be minimized by implementing hadoop, an open source solution. This can affect the support and maintenance costs of the business.
Modernizing workflows to big data open source environments expands the scope of future workflow updates, makes it easy to build systems that adapt to new technologies, and thus broadizes development methods
​
Mainframes are capable of creating virtual Linux servers. These servers can run most of the available open source products, being able to put a Linux cluster running hadoop on the same host that contains large data stores in various proprietary formats.
They can be created, moved, and removed with relative ease and with little disruption to the physical footprint of the infrastructure. As a result, if you are developing processes (i.e., applications) that can perform ETL internally, you can eliminate much of the complexity and cost of mainframe to non-mainframe ETL.
The "use on demand" nature of the mainframe allows scaling up and down with minimal disruption to the existing infrastructure and minimal impact on the physical footprint of the infrastructure. For example, we can double or triple the number of available clusters by issuing a few commands without disrupting the network
​
The authors argue that using hadoop on mainframes to analyze data can significantly reduce the cost of analytical computations and greatly improve compliance by avoiding the creation of unnecessary copies of data outside the source (mainframe) platform


						The data center evolution from Mainframe to Cloud
2016，5cite
云计算并没有扼杀大型机。虽然云计算具备颠覆性的技术，然而，它导致了大型机的进化。
现在大型机的最大优势是已经拥有了一台。如果您还没有拥有一个，那么几乎没有理由去投资一个，因为云计算提供的解决方案通常在几乎所有情况下都更具成本效益。
大型机的一个好处是可以100%完全控制自己的数据。当使用云服务时，第三方公司可能会碰触你的数据。使用大型机，不需要担心它们窥探或触摸用户数据。当然实际上，大多数大型云计算公司都是值得信赖的，这种可能性非常小。
然而，如果你已经有了一个，绝对有理由保留它。转移数十万行代码的成本可能会超过切换到云计算的好处。
与云服务相比，大型机具有更强的定制和专门化能力，因为硬件本身是由用户控制的。大型计算机减少了所使用的带宽，并且即使在互联网关闭时也可以使用

云计算的一个问题是它依赖于一个相当强大的互联网连接，希望这个连接永远不会宕机，或者宕机尽可能少，以证明连接中断所带来的成本是合理的。但这应该是一个罕见的问题，因为今天大多数企业都能保持良好的连接。

大型机市场依然强劲的原因有两个方面。一种是锁定——在大型机上运行的旧的业务关键型应用程序，其中内置了多年的业务逻辑，迁移到其他地方需要花费大量资金。
另一个是大型机在现代混合云架构中的相关性。多年来，主机已经发展了很多，可以在大型机上运行Linux。这些大型提供商正在做的很多事情实际上是试图通过将许多服务器散列在一起来重新创建大型机。
						
Cloud computing did not kill the mainframe. While cloud computing is a disruptive technology, it has led to the evolution of mainframes.
The big advantage of a mainframe now is that you already have one. If you do not already own one, there is little reason to invest in one, as cloud computing provides solutions that are generally more cost effective in almost all cases.
One of the benefits of a mainframe is that you have 100% complete control over your own data. When using cloud services, third-party companies may have access to your data. With mainframes, you don't need to worry about them snooping or touching user data. In practice, of course, it's highly unlikely that most large cloud computing companies are trustworthy.
However, if you already have one, there are definitely reasons to keep it. The cost of shifting hundreds of thousands of lines of code can outweigh the benefits of switching to cloud computing.
Compared to cloud services, mainframes have greater customization and specialization capabilities because the hardware itself is controlled by the user. Mainframe computers reduce the bandwidth used and can be used even when the Internet is off
​
One of the problems with cloud computing is that it relies on a fairly strong Internet connection, and hopefully this connection will never go down, or it will go down as little as possible, in order to justify the cost imposed by a broken connection. But this should be a rare problem, as most businesses today stay well connected.
​
The reason the mainframe market remains strong is twofold. One is lock-in - old business-critical applications running on mainframes with years of business logic built into them that cost a lot of money to migrate elsewhere.
Another is the relevance of mainframes in modern hybrid cloud architectures. Mainframes have evolved a lot over the years to run Linux on mainframes. A lot of what these large providers are doing is actually trying to recreate the mainframe by hashing many servers together.





						Metacomputing
1992,515cite
构建元计算机的第一阶段主要是软件和硬件的集成工作，它包括将所有资源与高性能网络互连，实现分布式文件系统，协调跨系统的用户访问
元 计算机发展的下一个阶段超越了异构计算机网络的软件集成。第二阶段涉及将单个应用程序扩展到多台计算机上，允许中心的异构计算机集合协同工作以解决单个问题。这使用户能够尝试没有元计算机几乎不可能实现的各种计算。允许以一般方式完成此操作的软件(与一次性、临时解决方案相反)刚刚出现，并且随着用户开始使用它，正在进行评估和改进。
元计算机发展的第三阶段将是一个透明的国家网络，它将极大地增加应用程序可用的计算和信息资源。这个阶段涉及的不仅仅是让本地元计算机使用远程资源(即，改变组件之间的距离)。第三阶段包括建立适当的WAN基础设施和开发管理、文件系统和安全方面的标准。
作者以NCSA的元计算机作为第一阶段的范例，以SIGGRAPH在92年的文献作为第二阶段开始的范例，在理论模拟，分子虚拟现实等方面展示了元计算的部分成果

The first phase of building the meta-computer is mainly the integration work of software and hardware, which includes interconnecting all resources with high performance networks, implementing distributed file systems, and coordinating user access across systems
The next stage in the evolution of the metacomputer goes beyond the software integration of heterogeneous computer networks. The second phase involves scaling a single application onto multiple computers, allowing a central collection of heterogeneous computers to work together to solve a single problem. This enables users to attempt a variety of computations that would be nearly impossible without the meta-computer. Software that allows this to be done in a general way, as opposed to one-off, AD hoc solutions, has just emerged and is being evaluated and improved as users begin to use it.
The third stage in the development of the meta-computer will be a transparent national network that will dramatically increase the computational and information resources available to applications. This phase involves more than just making remote resources available to the local metacomputer (that is, changing the distance between components). The third phase consists of setting up an appropriate WAN infrastructure and developing standards in management, file systems, and security.
The authors take NCSA's metasystem as an example in the first stage and SIGGRAPH's paper in 1992 as an example in the beginning of the second stage, and show some achievements of metasystem in theoretical simulation, molecular virtual reality and so on

						The Grid: A New Infrastructure for 21st Century Science
1998，5774cite
衡量技术变革速度的一个有用指标是，速度或容量翻番或价格减半的平均时间。对于存储、网络和计算能力，这些周期分别为12个月、9个月和18个月左右。
在这篇文章中，作者认为计算机速度每18个月翻一番的速度无法跟上增长更快的存储和传输，探讨了"网格计算"这一概念
作者提到，身份验证、授权和策略是网格中最具挑战性的问题。传统的安全技术主要关注保护客户机和服务器之间的交互，而在网格环境中，情况更为复杂，客户机和服务器之间的区别趋于消失
单点登录(用户应该能够进行一次身份验证，然后为计算分配操作的权利)，映射到本地安全机制(不同的站点可能使用不同的本地安全解决方案,网格安全基础设施需要映射到每个站点上的这些本地解决方案)，代理凭证(必须仔细管理这些委托操作和启用它们的授权证书)，社区授权和策略(每个资源都要跟踪社区成员和特权是不可行的,需要能够根据其他标准来表达策略)

A useful measure of the speed of technological change is the average time it takes for speed or capacity to double or for prices to halve. These periods are around 12, 9 and 18 months for storage, network and computing power, respectively.
In this article, the author discusses the concept of "grid computing", arguing that the doubling of computer speeds every 18 months cannot keep up with the faster growth of storage and transfer
The authors mentioned that authentication, authorization, and policies are the most challenging issues in the grid. Traditional security techniques mainly focus on protecting the interaction between client and server, while in grid environment, the situation is more complex and the distinction between client and server tends to disappear
Single sign-on (users should be able to authenticate once and then assign rights to operations for computation), map to local security mechanisms (different sites may use different local security solutions, and grid security infrastructure needs to map to these local solutions on each site), Proxy credentials (these delegated actions and the authorization certificates that enable them must be carefully managed), community authorizations and policies (it is not feasible for every resource to keep track of community membership and privileges, and it is necessary to be able to express policies based on other criteria)


						A Resource Management Architecture for Metacomputing Systems
1998,929cite
元计算系统旨在支持远程和/或并发使用地理上分布的计算资源。元计算系统允许应用程序根据需要组装和使用计算资源集合，而不考虑物理位置。
元计算环境引入了五个具有挑战性的资源管理问题:站点自治、异构基础、策略可扩展性、共同分配和在线控制
作者描述了一个解决这些问题的资源管理体系结构。该体系结构将资源管理问题分布在不同的本地管理器、资源代理和资源共同分配器组件之间，并定义了一种可扩展的资源规范语言来交换有关需求的信息。

站点自治问题指的是资源通常由不同的组织在不同的管理域中拥有和操作；
异构基板问题源于站点自治问题，是指不同站点可能使用不同的本地资源管理系统；
策略可扩展性问题的出现是因为元计算应用程序来自广泛的领域，每个领域都有自己的需求；
共同分配问题是因为许多应用程序的资源需求只能通过在几个站点同时使用资源来满足。站点自治和在分配过程中出现故障的可能性导致需要专门的机制来分配多个资源；
在线控制问题之所以出现，是因为可能需要进行大量协商，以使应用程序需求适应资源可用性，特别是在执行过程中需求和资源特征发生变化时；

先前关于元计算系统资源管理的工作可以分为两大类:
网络批处理排队系统。这些系统严格关注一组联网计算机的资源管理问题。批调度系统提供了一种有限形式的策略可扩展性
广域调度系统。在这里，资源管理作为将应用程序组件映射到资源并调度其执行的组件来执行。到目前为止，这些系统没有解决异构底物、场地自治和共同分配的问题。

文章定义了一种资源规范语言(RSL)作为组件之间通信对资源的请求,并构造了一个多层的资源体系结构(从应用程序到资源代理，到资源共同分配器和资源管理器)
资源代理负责获取高级RSL规范，并通过作者称为专门化的过程将其转换为更具体的规范
这样的地面请求可以传递给共同分配器，该分配器负责协调多个站点的资源分配和管理
资源共同分配器将多请求分解为其组成元素，并将每个组件传递给适当的资源管理器


Meta-computing systems are designed to support remote and/or concurrent use of geographically distributed computing resources. Metacomputing systems allow applications to assemble and use collections of computing resources as needed, regardless of physical location.
The metacomput environment introduces five challenging resource management issues: site autonomy, heterogeneous foundation, policy scalability, co-allocation, and online control
The authors describe a resource management architecture that addresses these issues. The architecture distributes the resource management problem among different local managers, resource brokers, and resource common allocator components, and defines an extensible resource specification language to exchange information about requirements.
​
The site autonomy problem refers to the fact that resources are usually owned and operated by different organizations in different administrative domains.
The heterogeneous substrate problem originates from the site autonomy problem, which means that different sites may use different local resource management systems.
Policy scalability issues arise because metacomputer applications come from a wide range of domains, each with its own requirements;
The co-allocation problem arises because the resource requirements of many applications can only be met by using resources simultaneously at several sites. Site autonomy and the possibility of failures during the allocation process lead to the need for specialized mechanisms to allocate multiple resources.
Online control problems arise because a lot of negotiation may be required to adapt application requirements to resource availability, especially when requirements and resource characteristics change during execution;
​
Prior work on resource management in metacomputer systems can be divided into two broad categories:
Network batch queuing system. These systems strictly focus on the problem of resource management for a group of networked computers. Batch scheduling systems provide a limited form of policy scalability
Wide area scheduling system. Here, resource management is performed as a component that maps application components to resources and schedules their execution. So far, these systems do not address the issues of heterogeneous substrates, site autonomy, and co-allocation.
​
This paper defines a resource specification language (RSL) to communicate resource requests between components, and constructs a multi-tier resource architecture (from application to resource broker, to resource co-allocator and resource manager).
The resource agent is responsible for taking the high-level RSL specification and transforming it into a more specific specification through a process the authors call specialization
Such ground requests can be passed to the common allocator, which coordinates resource allocation and management across multiple sites
The resource co-allocator decomposes multiple requests into their constituent elements and passes each component to the appropriate resource manager

   						The Harness Metacomputing Framework
1999,45cite
Harness是一个实验性的元计算框架，它基于动态可重构的原则，不仅针对组成虚拟机的计算机和网络，而且还针对虚拟机本身的功能。Harness的这个基本特性旨在解决当前元计算框架的不灵活性，以及它们无法合并新技术和避免快速过时的问题
分布式和集群计算技术经常随着新的机器功能、互连网络类型、协议和应用程序需求而变化，底层中间件要么需要更改，要么需要重建，从而增加了所涉及的工作量，并阻碍了互操作性。

这个框架支持重新配置组成虚拟机的计算机和网络，还支持重新配置虚拟机本身的功能。这些特征可以通过作为系统中心特征的“插件”机制在用户控制下进行修改，提供一个可以动态适应以满足应用程序需求的虚拟机环境
Harness元计算框架中的基本抽象是分布式虚拟机(DVM)，用户可以通过插件的方式配置，加入或离开DVM


相关工作：
	PVM是最早用具体的虚拟机和编程环境术语提出元计算概念，并探索异构网络计算的系统之一。PVM基于动态的、用户指定的主机池的概念，软件在此基础上模拟通用的并发计算资源。
动态进程管理与PVM中的强类型异构消息传递相结合，为分布式内存并行程序提供了一个有效的环境。然而，PVM在许多方面缺乏灵活性，这可能会限制下一代元计算和协作应用程序的发展。例如，不支持多个DVM合并和分裂。两个不同的用户不能在一个活动的PVM机器中交互、协作和共享资源和程序。PVM使用互联网协议，这可能会妨碍使用专门的网络硬件
	Legion是一个元计算系统，可以容纳地理上分布的高性能机器和工作站的异构组合。Legion是一个面向对象的系统，其重点是提供对企业级分布式计算框架的透明访问。因此，它并不试图迎合不断变化的需求，并且在它所支持的计算模型类型和实现中都是相对静态的。
	Globus是建立在“Nexus”通信框架之上的元计算基础设施。Globus系统是围绕工具包的概念设计的，该工具包由与通信、资源分配、数据等相关的预定义模块组成。然而，这些模块的组装不应该像在Harness中那样在运行时动态地发生。
几乎所有上述项目都设想了一个模型，在这个模型中，非常高性能的模块被静态地连接起来，以构建一个更大的系统。Harness项目的主要思想之一是通过动态连接、断开连接和重新配置异构组件来交换一些效率，以获得增强的全局可用性、可升级性和故障恢复能力。

Harness is an experimental meta-computing framework that is based on the principle of dynamic reconfiguration, not only for the computers and networks that compose a virtual machine, but also for the functionality of the virtual machine itself. This fundamental feature of Harness aims to address the inflexibility of current metacomponent frameworks and their inability to incorporate new technologies and avoid rapid obsolescence
Distributed and cluster computing technologies often change with new machine capabilities, interconnection network types, protocols, and application requirements, and the underlying middleware either needs to be changed or rebuilt, increasing the amount of effort involved and hindering interoperability.
​
This framework supports reconfiguring the computers and networks that make up the virtual machine, as well as reconfiguring the functionality of the virtual machine itself. These characteristics can be modified under user control through a "plug-in" mechanism that serves as a central feature of the system, providing a virtual machine environment that can be dynamically adapted to meet application requirements
The basic abstraction in the Harness meta-computing framework is the Distributed Virtual Machine (DVM). Users can configure, join or leave the DVM by means of plug-ins
​
​
Related work:
PVM is one of the first systems to propose the concept of meta-computing in concrete virtual machine and programming environment terms and to explore heterogeneous network computing. PVM is based on the concept of a dynamic, user-specified pool of hosts on which software simulates generic concurrent computing resources.
Dynamic process management combined with strongly typed heterogeneous message passing in PVM provides an efficient environment for distributed memory parallel programs. However, PVM lacks flexibility in a number of ways, which may limit the development of the next generation of meta-computing and collaborative applications. For example, multiple DVM merging and splitting is not supported. Two different users cannot interact, collaborate, and share resources and programs in an active PVM machine. PVM uses Internet protocols, which may preclude the use of specialized network hardware
Legion is a meta-computing system that can accommodate a heterogeneous combination of geographically distributed high performance machines and workstations. Legion is an object-oriented system with a focus on providing transparent access to enterprise-class distributed computing frameworks. As such, it does not attempt to cater to changing requirements and is relatively static in both the types of computational models and implementations it supports.
Globus is a meta-computing infrastructure built on top of the "Nexus" communication framework. The Globus system is designed around the concept of a toolkit consisting of predefined modules related to communication, resource allocation, data, etc. However, the assembly of these modules should not happen dynamically at runtime as in Harness.
Almost all of the above projects envisage a model in which very high performance modules are statically connected to build a larger system. One of the main ideas of the Harness project is to trade some efficiency by dynamically connecting, disconnecting, and reconfiguring heterogeneous components for enhanced global availability, upgradability, and failure resilience.


todo：自动切割程序分割到分布式平台

						MapReduce: Simplified Data Processing on Large Clusters
2004，14048cite；2008，22950cite	

这是谷歌的一项工作，MapReduce 是一种编程模型和关联的实现，用于处理和生成大规模数据集。用户定义 Map 函数来处理输入数据，并生成一组中间键值对，然后定义 Reduce 函数来处理这些中间键值对。
MapReduce 自动处理数据分割、分发和故障恢复，使得程序可以在分布式系统中高效运行。	

作者设计了一个新的抽象，它允许用户表达试图执行的简单计算，但隐藏了库中并行化、容错、数据分布和负载平衡的混乱细节。
作者解释道，大多数计算涉及到对输入中的每个逻辑记录应用map操作，以计算一组中间键值对，然后对共享同一键的所有值应用reduce操作，以便适当地组合派生数据。使用带有用户指定map和reduce操作的功能模型，能够轻松地并行化大型计算，并使用重执行作为容错的主要机制。
这项工作的主要贡献是一个简单而强大的接口，它可以实现大规模计算的自动并行化和分布，并结合该接口的实现，在大型商用pc集群上实现高性能。该编程模型还可以用于在同一台机器的多个核心之间并行化计算。

用户程序中的MapReduce库首先将输入文件分成M个块，每个块通常为16-64MB(由用户通过可选参数控制)。然后，它在一组机器上启动该程序的许多副本。
该程序的一个副本master是特别的。其余的是由master分配工作。有M个map任务和R个reduce任务要分配。
被分配映射任务的工作线程读取相应输入分割的内容。它从输入数据中解析键值对，并将每对传递给用户定义的map函数，map函数产生的中间键值对在内存中进行缓冲。
周期性地将缓冲对写入本地磁盘，并通过分区函数划分为R个区域。这些缓冲对在本地磁盘上的位置被传递回主服务器，主服务器负责将这些位置转发给reduce worker
当主服务器通知reduce worker有关这些位置时，它使用远程过程调用从map worker的本地磁盘读取缓冲数据。当reduce worker读取了其分区的所有中间数据后，它按中间键对数据进行排序，以便将出现的所有相同键分组在一起
reduce worker遍历已排序的中间数据，对于遇到的每个唯一的中间键，它将键和相应的中间值集传递给用户的reduce函数。reduce函数的输出被附加到这个reduce分区的最终输出文件中
主程序唤醒用户程序。此时，用户程序中的MapReduce调用返回到用户代码

拓展功能
用户指定的分区函数，用于确定中间键值到R reduce分片的映射
排序保证:我们的实现保证在每个R reduce分区中，中间键/值对以递增的键顺序处理
用户指定的组合函数，用于在相同的map任务中使用相同的键对生成的中间值进行部分组合(以减少必须通过网络传输的中间数据量)
自定义输入和输出类型，用于读取新的输入格式和产生新的输出格式
在一台机器上执行的模式，以简化调试和小规模测试。

This is an effort by Google, MapReduce is a programming model and associative implementation for processing and generating large-scale datasets. The user defines the Map function to process the input data and generate a set of intermediate key-value pairs, and then defines the Reduce function to process these intermediate key-value pairs.
MapReduce automatically handles data partitioning, distribution, and failure recovery, allowing programs to run efficiently in distributed systems.
​
The authors design a new abstraction that allows the user to express the simple computation that is trying to be performed, but hides the messy details of parallelization, fault tolerance, data distribution, and load balancing in the library.
The authors explain that most computations involve applying a map operation to each logical record in the input to compute a set of intermediate key-value pairs, and then applying a reduce operation to all values sharing the same key in order to properly combine the derived data. Using a functional model with user-specified map and reduce operations enables easy parallelization of large computations and uses reexecution as the primary mechanism for fault tolerance.
The main contribution of this work is a simple yet powerful interface that enables automatic parallelization and distribution of large-scale computations, combined with the implementation of this interface to achieve high performance on large commodity pc clusters. The programming model can also be used to parallelize computations among multiple cores on the same machine.
​
The MapReduce library in the user program starts by splitting the input file into M chunks, each of which is typically 16-64MB in size (controlled by the user via optional parameters). It then launches many copies of the program on a set of machines.
One copy of the program, master, is special. The rest is for the master to assign the work. There are M map tasks and R reduce tasks to allocate.
The worker thread assigned the mapping task reads the contents of the corresponding input split. It parses key-value pairs from the input data and passes each pair to a user-defined map function, which produces intermediate key-value pairs that are buffered in memory.
The buffer pairs are periodically written to the local disk and divided into R regions by the partitioning function. The locations of these buffer pairs on the local disk are passed back to the master server, which is responsible for forwarding these locations to the reduce worker
When the master informs the reduce worker about these locations, it reads the buffered data from the map worker's local disk using a remote procedure call. When the reduce worker has read all the intermediate data for its partition, it sorts the data by the intermediate key in order to group together all the occurrences of the same key
The reduce worker iterates over the sorted intermediate data, and for each unique intermediate key it encounters, it passes the key and the corresponding set of intermediate values to the user's reduce function. The output of the reduce function is appended to the final output file of this reduce partition
The main program wakes up the user program. At this point, the MapReduce call in the user program returns to the user code
​
Extending functionality
User-specified partitioning function to determine the mapping of intermediate keys to R reduce shards
Ordering guarantee: Our implementation guarantees that in each R reduce partition, intermediate key/value pairs are processed in increasing key order
User-specified composition function to partially combine intermediate values generated using the same key in the same map task (to reduce the amount of intermediate data that must be transferred over the network)
Custom input and output types for reading new input formats and producing new output formats
Mode of execution on a single machine to simplify debugging and small-scale testing.


						Apache Hadoop YARN: Yet Another Resource Negotiator
2013，2844cite
Apache Hadoop最初是MapReduce的众多开源实现之一。开发人员为了充分利用物理资源，经常采用巧妙的变通方法来避开MapReduce API的限制。
这些限制和误用激发了一整类将Hadoop作为不相关环境基准的论文。到目前为止，学术界和开源社区都很好地理解了原始Hadoop架构的局限性

作者介绍了下一代Hadoop计算平台YARN，与熟悉的单片架构不同，通过将资源管理功能与编程模型分离，YARN将许多与调度相关的功能委托给每个作业组件。在这个新的上下文中，MapReduce只是运行在YARN之上的应用程序之一。这种分离为选择编程框架提供了很大的灵活性

YARN由资源管理器，节点管理器，应用管理器和容器构成，资源管理器分配资源，节点管理器处理节点并监控节点中资源使用情况，容器包含物理资源的集合
每个集群的ResourceManager (RM)跟踪资源使用情况和节点活跃度，执行分配不变量，并仲裁租户之间的争用。作为专用机器上的守护进程运行，作为集群中各种竞争应用程序之间的资源仲裁中心。它可以跨租户强制执行属性，如公平性、容量和局部性。根据应用程序需求、调度优先级和资源可用性，RM动态地将租约(称为容器)分配给在特定节点上

ApplicationMaster (AM)通过从RM请求资源、从它接收到的资源生成物理计划以及围绕故障协调该计划的执行来协调单个作业的逻辑计划。
为了执行和跟踪这些分配，RM与运行在每个节点上的称为NodeManager (NM)的特殊系统守护进程进行交互。RM和NMs之间的通信是基于心跳的，以实现可伸缩性。NMs负责监控资源可用性、报告故障和容器生命周期管理。

当某个作业被生成时，作业通过一个公共提交协议提交给RM，并经过一个准入控制阶段，在此阶段，安全凭证被验证，各种操作和管理检查被执行。接受的作业被传递给要运行的调度程序。一旦调度器拥有足够的资源，应用程序就会从接受状态转到运行状态。为AM分配一个容器，并在集群中的一个节点上生成它。接受的应用程序的记录被写入持久存储，并在RM重新启动或出现故障时恢复。
Applicatio nMaster是作业的“头”，管理所有生命周期方面，包括动态增加和减少资源消耗，管理执行流(例如，根据映射的输出运行reducer)，处理故障和计算倾斜，以及执行其他局部优化。实际上，AM可以运行任意用户代码，并且可以用任何编程语言编写，因为与RM和NM的所有通信都是使用可扩展通信协议进行编码的
为了获取容器，AM向RM发出资源请求。当代表AM分配资源时，RM为该资源生成一个租约，该租约由后续的AM心跳提取。当AM向NM提交容器租约时，基于令牌的安全机制保证其真实性

Apache Hadoop was originally one of many open source implementations of MapReduce. Developers often work around the limitations of the MapReduce API by using clever works to get the most out of their physical resources.
These limitations and misuses have inspired an entire class of papers that use Hadoop as a benchmark for unrelated environments. By now, the limitations of the original Hadoop architecture are well understood by both academic and open source communities
​
The authors introduce YARN, a next-generation Hadoop computing platform that, unlike the familiar monolithic architecture, delegates many scheduling-related functions to each job component by separating resource management functions from the programming model. In this new context, MapReduce is just one of the applications running on top of YARN. This separation provides a great deal of flexibility in the choice of programming framework
​
YARN is composed of a resource manager, a node manager, an application manager, and containers. The resource manager allocates resources, the node manager processes nodes and monitors their resource usage, and the container contains a collection of physical resources
The ResourceManager (RM) of each cluster tracks resource usage and node activity, enacts allocation invariants, and arbitrates contention among tenants. Running as a daemon on a dedicated machine, it acts as a resource arbitration center between the various competing applications in the cluster. It can enforce properties such as fairness, capacity, and locality across tenants. The RM dynamically assigns leases, called containers, to specific nodes based on application requirements, scheduling priorities, and resource availability
​
The ApplicationMaster (AM) coordinates the logical plan of a single job by requesting resources from the RM, generating a physical plan from the resources it receives, and coordinating the execution of this plan around failures.
To execute and track these allocations, the RM interacts with a special system daemon called NodeManager (NM) running on each node. The communication between RM and NMs is heartbeat-based to achieve scalability. The NMs is responsible for monitoring resource availability, reporting failures, and container lifecycle management.
​
When a certain job is generated, the job is submitted to the RM through a common submission protocol and goes through an admission control phase where security credentials are verified and various operational and administrative checks are performed. Accepted jobs are passed to the scheduler to be run. Once the scheduler has enough resources, the application moves from accepting to running state. Allocate a container for AM and generate it on a node in the cluster. Records of accepted applications are written to persistent storage and recovered upon RM restarts or failures.
Applicatio nMaster is the "head" of the job and manages all lifecycle aspects, including dynamically increasing and decreasing resource consumption, managing execution flow (e.g., running reducers based on mapped outputs), handling failures and computation skew, and performing other local optimizations. Indeed, AM can run arbitrary user code and can be written in any programming language, since all communications with RM and NM are encoded using extensible communication protocols
To acquire the container, AM makes a resource request to RM. When allocating a resource on behalf of an AM, the RM generates a lease for that resource, which is extracted by subsequent AM heartbeats. When AM submits a container lease to NM, the token-based security mechanism guarantees its authenticity

						Bigtable: A Distributed Storage System for Structured Data
@article{chang2008bigtable,
  title={Bigtable: A distributed storage system for structured data},
  author={Chang, Fay and Dean, Jeffrey and Ghemawat, Sanjay and Hsieh, Wilson C and Wallach, Deborah A and Burrows, Mike and Chandra, Tushar and Fikes, Andrew and Gruber, Robert E},
  journal={ACM Transactions on Computer Systems (TOCS)},
  volume={26},
  number={2},
  pages={1--26},
  year={2008},
  publisher={ACM New York, NY, USA}
}
2008，8061cite
作者设计、实现并部署了一个分布式存储系统
Bigtable是一个分布式存储系统，用于管理结构化数据，其设计用于扩展到非常大的规模:数千台商用服务器上的pb级数据。
Bigtable旨在可靠地扩展到pb级数据和数千台机器，已经实现了几个目标:广泛的适用性、可伸缩性、高性能和高可用性。

Bigtable不支持完整的关系数据模型;相反，它为客户机提供了一个简单的数据模型，该模型支持对数据布局和格式的动态控制，并允许客户机推断底层存储中表示的数据的局部性属性。
Bigtable还将数据视为未解释的字符串，尽管客户端经常将各种形式的结构化和半结构化数据序列化到这些字符串中。
最后，Bigtable模式参数允许客户端动态控制从内存还是从磁盘提供数据。

Bigtable集群是运行Bigtable软件的一组进程。每个集群服务于一组表。Bigtable中的表是一个稀疏的、分布式的、持久的多维排序映射。数据被组织成三个维度:行、列和时间戳。
我们将特定行键、列键和时间戳所引用的存储称为单元格。将行分组在一起以形成负载平衡单元，将列分组在一起以形成访问控制和资源计费单元。

经验教训：
大型分布式系统容易受到许多类型的故障的影响，而不仅仅是许多分布式协议中假定的标准网络分区和故障停止故障。
推迟添加新功能是很重要的，直到清楚新功能将如何使用。
最重要的一课是简单设计的价值。考虑到系统的大小(大约100,000行非测试代码)，以及代码随着时间以意想不到的方式发展的事实，可以发现代码和设计的清晰度对代码维护和调试有巨大的帮助。
必须解决无共享数据库所面临的一些相同类型的负载和内存平衡问题。
解决方法
(1)不考虑相同数据的多个副本的可能性，可能由于视图或索引而以不同的形式
(2)让用户告诉我们哪些数据属于内存，哪些数据应该留在磁盘上，而不是试图动态地确定这一点
(3)没有复杂的查询需要执行或优化。

The author designs, implements and deploys a distributed storage system
Bigtable is a distributed storage system for managing structured data that is designed to scale to a very large size: petabytes of data across thousands of commodity servers.
Bigtable is designed to reliably scale to petabytes of data and thousands of machines. Bigtable has achieved several goals: wide applicability, scalability, high performance, and high availability. 


Bigtable does not support a full relational data model; instead, it provides clients with a simple data model that supports dynamic control over data layout and format, and allows clients to reason about the locality properties of the data represented in the underlying storage.
Bigtable also treats data as uninterpreted strings, although clients often serialize various forms of structured and semi-structured data into these strings. 
Finally, Bigtable schema  parameters let clients dynamically control whether to serve data out of memory or from disk

A Bigtable cluster is a set of processes that run the Bigtable software. Each cluster serves a set of tables. A table in Bigtable is a sparse, distributed, persistent multidimensional sorted map. Data is organized into three dimensions: rows, columns, and timestamps.
We refer to the storage referenced by a particular row key, column key, and timestamp as a cell. Rows are grouped together to form the unit of load balancing, and columns are grouped together to form the unit of access control and resource accounting.

lessons learned：
One lesson we learned is that large distributed systems are vulnerable to many types of failures, not just the standard network partitions and fail-stop failures assumed in many distributed protocols. 
Another lesson we learned is that it is important to delay adding new features until it is clear how the new features will be used.
The most important lesson we learned is the value of simple designs. Given both the size of our system (about 100,000 lines of non-test code), as well as the fact that code evolves over time in unexpected ways, we have found that code and design clarity are of immense help in code maintenance and debugging.

Bigtable’s load balancer has to solve some of the same kinds of load and memory balancing problems faced by shared-nothing databases . 
Our problem is somewhat simpler: 
(1) we do not consider the possibility of multiple copies of the same data, possibly in alternate forms due to views or indices; 
(2) we let the user tell us what data belongs in memory and what data should stay on disk, rather than trying to determine this dynamically; 
(3) we have no complex queries to execute or optimize.


						Boxwood: Abstractions as the Foundation for Storage Infrastructure
@inproceedings{maccormick2004boxwood,
  title={Boxwood: Abstractions as the Foundation for Storage Infrastructure.},
  author={MacCormick, John and Murphy, Nick and Najork, Marc and Thekkath, Chandramohan A and Zhou, Lidong},
  booktitle={OSDI},
  volume={4},
  pages={8--8},
  year={2004}
}
2004，279cite
实现分布式的、可靠的、存储密集型的软件(如文件系统或数据库系统)是很困难的。
这些系统必须处理几个挑战，包括:将用户抽象(例如，文件、目录、表和索引)与底层存储提供的抽象相匹配，设计合适的数据放置、预取和缓存策略，以及提供足够的容错性、增量可伸缩性和易于管理性。
通过使用由存储子系统直接提供的合适的抽象(如树、链表和哈希表)，可以大大降低这种感知到的困难，而不会影响存储系统的性能、可伸缩性或构建在其上的更高级别子系统的可管理性。
尽管Boxwood的存储方法与磁盘提供的传统面向块的接口有很大的不同，但它提供了一些关键的优势。
一个优点是，通过将数据结构直接集成到持久存储体系结构中，可以更简单地构建高级应用程序，同时以很小的成本获得容错、分布和可伸缩性的好处。
此外，本质上可以处理稀疏和非连续存储的抽象使高级软件免于处理地址空间或空闲空间管理。
第三个优点是，使用数据抽象中固有的结构信息可以使系统执行更好的负载平衡、数据预取和知情缓存。这些机制可以在基础设施中实现一次，而不必在每个子系统或应用程序中重复。

作者认为，B树这种更简单的抽象为客户机应用程序提供了良好的性能和很大的灵活性，同时卸载了空闲空间(或在虚拟磁盘环境中，地址空间)管理的细节。

boxwood系统被组织为几个相互依存的服务。作者使用分层作为管理boxwood系统复杂性的一种方式。例如，前面提到的b树和块存储服务是通过将前者分层到后者之上来构建的。
反过来，块存储服务是在一个简单的复制逻辑设备抽象之上分层的。尽管由于上下文切换开销，分层有可能降低性能，但可以通过在单个地址空间内运行所有层来避免这些问题。

作者总结道，初步经验表明，使用可伸缩的数据抽象作为基本的、低级的存储原语是有吸引力的。
可以肯定的是，似乎很难确定一个单一的，普遍的抽象，满足所有的需求。然而，对抽象和服务的特殊组合似乎提供了一个良好的基础，在此基础上可以很容易地构建多个抽象。
将块管理器用作通用存储分配器，从而避免了系统中其他地方对地址空间管理的需要，这似乎是广泛适用的。


Implementing distributed, reliable, storage-intensive software such as file systems or database systems is hard.
These systems have to deal with several challenges including: matching user abstractions (e.g., files, directories, tables, and indices) with those provided by the underlying storage, designing suitable data placement, prefetching, and caching policies, as well as providing adequate fault-tolerance, incremental scalability, and ease of management.
Our hypothesis in this paper is that this perceived difficulty can be considerably lessened through the use of suitable abstractions such as trees, linked lists, and hash-tables, provided directly by the storage subsystem, without compromising performance, scalability, or the manageability of the storage system or the higher-level subsystems built on top of it.
Although Boxwood’s approach to storage is a significant departure from traditional block-oriented interfaces provided by disks—whether physical, logical [17], or virtual [21]—we think it provides some key advantages.
by directly integrating data structures into the persistent storage architecture, higher-level applications are simpler to build, while getting the benefits of fault-tolerance, distribution, and scalability at little cost. Furthermore, abstractions that can inherently deal with sparse and non-contiguous storage free higher level software from dealing with addressspace or free-space management. 
A third advantage is that using the structural information inherent in the data abstraction can allow the system to perform better load-balancing, data prefetching, and informed caching. These mechanisms can be implemented once in the infrastructure instead of having to be duplicated in each subsystem or application.

we believe that our simple chunk store abstraction provides a better match for applications that do not need the strict atomicity guarantees or the rigid structure of a B-tree. This simpler abstraction offers good performance and much flexibility to client applications while offloading the details of free space (or in a virtual disk environment, address space) management.
The Boxwood system is organized as several interdependent services. We use layering as a way of managing the complexity in a Boxwood system. For example, the B-tree and the chunk store services mentioned earlier in Section 1 are constructed by layering the former on top of the latter. The chunk store service, in turn, is layered on top of a simple replicated logical device abstraction (to be described in Section 3.4). Although layering has the potential for reducing performance because of context switching overheads, our design avoids these problems by running all layers within a single address space.

Our initial experience indicates that using scalable data abstractions as fundamental, low-level storage primitives is attractive. To be sure, it appears difficult to settle on a single, universal abstraction that will fit all needs. However, our particular combination of abstractions and services seem to offer a sound substrate, on top of which multiple abstractions may be readily built.

Our use of the chunk manager as a generalized storage allocator obviating the need for address space management elsewhere in the system seems to be widely applicable. 



						Mariposa: a new architecture for distributed data
@inproceedings{stonebraker1994mariposa,
  title={Mariposa: A new architecture for distributed data},
  author={Stonebraker, Michael and Aoki, Paul M and Devine, Robert and Litwin, Witold and Olson, Michael},
  booktitle={Proceedings of 1994 IEEE 10th International Conference on Data Engineering},
  pages={54--65},
  year={1994},
  organization={IEEE}
}
1994，161cite

The fundamental objective of Mariposa is to unlfy the (up until now) disparate approaches of distributed DBMSs, cachingbased distributed file systems, deepstore file systems and object servers. Mariposa distributes data over a collection of sites that are connected by some form of local- or wide-area network.

A Mariposa database consists of Instances of objects in named classes, each containing a collection of attributes of specific data types.
Each class is divided into a collection of fragments, which are the unit of storage in the system. Storage sites may split or coalesce fragments as desired. Fragments can optionally have a distribution criteria which controls the logical composition of instances in the fragment. Fragments do not have a specific home and can move freely within the network
For example, if a fragment is being accessed frequently, one would expect its location to be moved from tertiary memory to disk, or even to main memory. When activity abates, its location would migrate to slower storage
The Mariposa approach to these objectives is to designate one representation which each storage manager must support, the so-called canonical representation, and a second optional private representation. As such, fast fragment movement can be accomplished by moving the canonical representation from one storage manager to another.

Each Mariposa site is locally autonomous, i.e., the site’s database administrator (DBA) .fully controls object storage at that site.

Lastly, Mariposa has a query optimizer which must generate a plan for solving a query.

As a result. the major aspects of Mariposa are: 
a rule system compking an engine and rule set 
fragment movement algorithms to allow mobile data 
a query optimizer and execution engine 
an approach to multiple copies for this environment 

However, Mariposa must deal with the following additional difficulties.

First, no site has perfect knowledge of global database state.
Second, the optimizer has the option of moving the query to the site where it thinks the data resides or bringing the data to the site of the query. The choice of which strategy to employ is an optimization decision.
Third, should Mariposa decide to move the data to the query, it must choose which copy to move or decide to make a new copy, again leading to an increase in complexity.
Fourth, we assume that Mariposa sites can be heterogeneous, that is, they can have differing speeds, capacities and capabilities. For example, it may not be possible to execute all operations at all sites because of limitations on storage capacity.

We have presented the architecture of Mariposa, a prototype data management system that unifies the best features of distributed operating system and distributed database management system research. Unlike previous distributed data management systems, Mariposa manages object storage in an environment of high data mobility and highly heterogeneous system capabilities while retaining high performance. In addition, Mariposa’s rulebased storage architecture preserves local site autonomy and gives site administrators considerable flexibility in specifying their storage policies.



						The Vertica Analytic Database: C-Store 7 Years Later

@article{lamb2012vertica,
  title={The vertica analytic database: C-store 7 years later},
  author={Lamb, Andrew and Fuller, Matt and Varadarajan, Ramakrishna and Tran, Nga and Vandier, Ben and Doshi, Lyric and Bear, Chuck},
  journal={arXiv preprint arXiv:1208.4173},
  year={2012}
}
2012,482cite

The Vertica Analytic Database (Vertica) is a distributed , massively parallel RDBMS system that commercializes the ideas of the C-Store project.
Vertica is designed for analytic workloads on modern hardware and its success proves the commercial and technical viability of large scale distributed databases which offer fully ACID transactions yet efficiently process petabytes of structured data.

Vertica was explicitly designed for analytic workloads rather than for transactional workloads.
Transactional workloads are characterized by a large number of transactions per second (e.g. thousands) where each transaction involves a handful of tuples. Most of the transactions take the form of single row insertions or modifications to existing rows. Examples are inserting a new sales record and updating a bank account balance.
Analytic workloads are characterized by smaller transaction volume (e.g. tens per second), but each transaction examines a significant fraction of the tuples in a table. Examples are aggregating sales data across time and geography dimensions and analyzing the behavior of distinct users on a web site.
As others have pointed out, it is possible to exceed the performance of existing one-size-fits-all systems by orders of magnitudes by focusing specifically on analytic workloads.

Like C-Store, Vertica physically organizes table data into projections, which are sorted subsets of the attributes of a table. 

end