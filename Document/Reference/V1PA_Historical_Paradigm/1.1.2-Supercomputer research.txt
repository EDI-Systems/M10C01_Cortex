						High Performance Computing- History of the Supercomputer
早期
1943	Colossus 是第一台可编程的数字电子计算机，二战时期为破译密码而秘密设计，对后世计算机影响不大
1945	Manchester Mark I 意义重大，它是第一台使用索引寄存器修改基址的机器
1950	MIT Whirlwind是第一台实时运行的计算机，使用视频显示进行输出(之前的都是批处理计算机)，为了应对这种需求，创建了核心存储器（在磁场极性中存储数据的铁氧体环）
1960	LARC 是第一台真正的超级计算机，它是为多处理而设计的
1961	7030  IBM为了击败LARC设计的计算机，有很多创造性举措得以沿用，例如多重编程、内存保护、广义中断、8 位字节、指令流水线、预取和解码以及内存交错
1965	CDC 6600 比当时的最快计算机快10倍，第一个RISC系统，具备10个外围处理器来处理IO和运行操作系统
向量时代
1974	CDC Star-100 是最早使用矢量处理器的机器之一，为了提高矢量性能，牺牲了基本的标量性能，机器通常被认为是失败的。
1975	Cray-1	使用矢量寄存器而不是流水线内存操作而不影响标量性能的矢量处理器。附带Cray操作系统、Cray 汇编语言 和 Cray FORTRAN，第一个自动矢量化的 FORTRAN 编译器
1981	CDC Cyber-205 纠正了 Star-100 的错误，使用半导体内存和虚拟内存概念
1983	Cray X-MP  并行矢量处理器机器，更好的链接支持、并行算术流水线和共享内存访问，每个处理器具有多个流水线。
传统时代 1991-2010
1995 	Intel ASCI Red 2.15 TFLOPS 
2000 	IBM ASCI White 7.226 TFLOPS 
2002 	Earth Simulator 35.86 TFLOPS 
2005 	IBM ASCI Blue Gene 70 -478 TFLOPS 
2008 	IBM Roadrunner 1.105 PFLOPS 
2009 	Cray Jaguar 1.75 PFLOPS 
GPU时代 2011-至今
2010 	Tianhe-1A 2.57 PFLOPS 
2011 	Fujitsu K computer 8.2 –10.5 PFLOPS 
2012 	IBM Sequoia 20.1 PFLOPS 
2013 	Tianhe-2 54.9 PFLOPS 
2015 	Sunway TaihuLight 125.4 PFLOPS 
2018 	Summit 187.6 PFLOPS 
2020 	Fugaku 415.5 PFLOPS 
2022 	Frontier 1102 PFLOPS
						The CRAY-1 Computer System
1976
历史上最成功的超级计算机，能够在持续周期内支持每秒1.38亿次浮点运算(MFLOPS)的计算速率。
通过一种叫做链接的技术，CRAY-1矢量功能单元与标量和矢量寄存器相结合，产生临时结果并立即再次使用它们，而不需要额外的内存引用，这在其他当时的计算机系统中减慢了计算过程。
使用链接技术，当从一个功能单元发出的结果(以一个/时钟周期的速率)立即馈送到另一个功能单元，以此类推。换句话说，中间结果不必存储到内存中，甚至可以在创建它们的vector操作运行完成之前使用。
链接类似于IBM计算机中使用的“数据转发”技术。与数据转发一样，链接也是自动进行的。
增强CRAY-1的计算能力的特点是:它的体积小（基座约0.5米高直径约2.5米，主体不到2米直径约1.5米），这减少了电信号必须在计算机框架内传播的距离，并允许12.5纳秒的时钟周期，拥有当时世界上最快的标量处理器。具有错误检测和纠错逻辑的100万字半导体存储器(SECD~D)，具有64位字长，及其优化的Fortran编译器。

						Reevaluating Amdahl's Law
1988
阿姆达尔定律：
优化前用时T1，优化后用时Tn，记F为程序中的串行比例，则(1-F)是并行比例，由此可得加速比为T1/Tn，将Tn=T1(F+(1-F)/n)代入化简得加速比s=1/(F+(1-F)/n)
可以从“加速比”的公式中看出，单纯地增加处理器的数量并不一定可以有效地提高系统的性能，只有在提高系统内并行化模块比重的前提下，同时合理增加处理器的数量，才能以最小的投入得到最大的加速比

作者基于此提出了古斯塔夫森定律：
记a为串行时间，b为并行时间，n为处理器个数，则加速比s=(a+nb)/(a+b)，由于串行比例F=a/(a+b)，变换后得到s=n-F(n-1)
可以看出，F（串行化程度）足够小，也即并行化足够高，那么加速比和cpu个数成正比。也是说明处理器个数、串行比例和加速比之前的关系，只不过它的侧重角度有所不同。
这两个定律是高并发程序中的重要定律，对并行计算的理论基础有重要贡献。
						What’s Next in High-Performance Computing
2002
超级计算机存在两种主要架构:克雷式向量超级计算机集群;以及标量单和多处理器集群。集群正在从运行专有软件的大规模并行计算机和集群过渡到运行标准软件的专有集群，以及从商用硬件和软件构建的自己动手的Beowulf集群。
它建立在几十年的并行处理研究和将松耦合计算机应用于各种应用的许多尝试之上。部分组件包括:消息传递接口(MPI)编程模型;并行虚拟机(PVM)编程，执行和调试模型;并行文件系统;配置、调度、管理和调整并行应用程序的工具;高级库，例如Linpack, BLAS。
Beowulf商品集群的缺点是，在需要大量共享内存的应用程序上表现不佳。
						High-Performance Computing: Clusters, Constellations, MPPs, and Future Directions
2005
基于上一篇文章给出的集群概念进行完善和修正，集群的定义范围限制为由独立节点的集成集合组成的并行计算机系统，每个节点都是一个独立的系统，能够独立运行，并且派生于为其他独立目的开发和销售的产品
例如，集群- now系统几乎完全使用消息传递接口(MPI)编程，而星座系统可能至少部分使用OpenMP编程，使用线程模型。通常，星座是空间共享的，而不是时间共享的，每个用户都有自己的节点;空间共享clusterNOW系统意味着为特定用户分配一定数量的节点。
强调描述并行计算系统的四个主要维度:集群、命名空间、并行性以及延迟和局部性管理

						Distributed Shared Memory: Concepts and Systems
1996
多处理器系统的内存系统组织，通常分为两大类:共享内存系统和分布式内存系统。
共享内存系统(通常称为紧耦合多处理器)使所有处理器都可以平等地访问全局物理内存。
具有简易性和可移植性。但是，共享内存多处理器在访问共享内存时通常会遇到争用增加和延迟延长的问题。分布式存储系统(通常称为多计算机)由多个独立的处理节点和本地存储模块组成，通过通用互连网络连接。由于必须的处理跨系统的数据分布和管理通信，以及进程迁移也会带来问题。因此，与共享内存系统相比，分布式内存系统中的硬件问题更容易，软件问题更复杂。
分布式共享内存结合了这两种方法的优点。
DSM系统在物理分布式内存系统上逻辑地实现共享内存模型。系统设计人员可以通过各种方式在硬件或软件中实现实现共享内存抽象的特定机制。
DSM系统一般结构DSM系统一般由一组节点或集群组成，通过互联网络连接。集群本身可以是单处理器或多处理器系统，通常围绕共享总线组织。
用于分发共享数据的两种常用策略是复制和迁移。复制允许同一数据项的多个副本驻留在不同的本地内存(或缓存)中。它主要用于使不同站点能够同时访问相同的数据，主要是在读取共享盛行的情况下。迁移意味着在任何时候只存在数据项的一个副本，因此必须将数据项移动到请求站点以独占使用。为了减少一致性管理开销，在普遍采用顺序写共享模式时，用户更喜欢这种策略。系统设计人员必须选择一种DSM算法，这种算法必须很好地适应典型应用程序中内存引用的系统配置和特征


						Exascale Computing Trends: Adjusting to the “New Normal” for Computer Architecture
2015
近20年来，持续的性能(以每秒浮点运算次数为单位)以每年约1.9倍的速度持续增长。
在2004年之前，这种增长来自内核数量的适度增加，加上内核时钟速率的大幅提高(每年提高50%或更高)，以及每个内核内存的大幅提高。但在2004年之后，内核的年增长率直线上升，而内核时钟的平均增长率消失了，每个内核的内存甚至下降了。

使用在exascale技术报告中首次引入的术语，这些术语包括重量级、轻量级和最近的混合。
重量级架构是2004年以前的系统的自然后代，这些系统有一个或多个传统的高端多核微处理器芯片，通常有各种支持芯片连接到相对大量的传统DRAM存储器双内联内存模块(dimm)，以及执行节点外通信和路由的芯片。通常，计算和路由芯片以非常高的时钟速率运行，并且需要大型(和“沉重”)的散热器来保持它们的冷却。现代重量级处理器插座中的芯片有8到16核，每个有2到4个fpu，运行在3ghz左右。
轻量级架构始于2004年的IBM Blue Gene/L2;它有双处理器内核，集成了内存控制器和I/O和路由功能在一个芯片上。这些核心比重量级机器上的核心简单得多，运行时的时钟速率也低得多。这样的芯片，当与存储器结合在一起时，就构成了一个完整的节点，两个这样的节点封装在小卡上，插到主板上，相互连接。随后的版本，Blue Gene/P3和现在的Blue Gene/Q,4都延续了这类架构。
第三类已经浮出水面;它将一个重量级处理器与第二个芯片结合在一起，第二个芯片拥有大量更简单的核心，通常每个核心都有更多的fpu，源自gpu。这些加速器与本地高速、低密度的存储器相连，以保存它们处理的所有数据，需要在重量级存储器之间进行传输
从本质上讲，在未来的技术世代中，即使是同质的硬件也会变得越来越异构。因此，不能再依赖于同质性，这对批量同步执行模型提出了存在的挑战。

						Computing beyond Moore’s Law
2015
摩尔定律的终结将对各种消费电子设备的封装和性能提出挑战，这些设备依赖于成本和能源效率的提高，以便在电池容量或电源有限的情况下，从设备中挤出更多的功能。如果智能手机行业不能在更小的空间内容纳更多的功能，那么智能手机的智能化能力就会受到影响。

更深层次的问题是对美国计算机行业增长的威胁。

摩尔定律将计算变成了一种普及的消费技术，在一个呈指数级增长的市场中变得越来越强大。这种规模的结束可能会减缓产品改进的步伐，这可能会对经济产生重大的负面影响。
这些挑战促使研究人员从更广泛的角度来看待计算的构成。情报高级研究项目活动(IARPA)最近委托撰写的一份报告《情报界替代计算技术的初步展望》，建议研究四种基本计算模型: 
经典数字计算(CDC)，其中包括构成计算和消费电子行业基础的所有二进制数字
电子模拟计算(AC)，包括通过直接物理原理实现计算的非二进制设备
神经启发计算(NC)，包括基于大脑操作原理和一般神经元计算的设备
量子计算(QC)，理论上可以通过从问题的所有可能答案的叠加中选择所需状态来解决一些具有组合复杂性的问题。
神经启发计算(NC)，包括基于大脑操作原理和一般神经元计算的设备;量子计算(QC)，理论上可以通过从问题的所有可能答案的叠加中选择所需状态来解决一些具有组合复杂性的问题。
架构和软件的进步：能源管理(细粒度的电源管理可能会提供额外的恢复能量的潜力)，电路设计(可以使电线在较低的电压下进行长距离连接，然后在端点有效地进行再放大，尽管再放大会造成一些损失)，片上系统(SoC)专门化，定制逻辑，近阈值电压操作



						GPU Cluster for High Performance Computing
2004，748cite
自20世纪90年代中期以来，在游戏产业的推动下，GPU性能大约每6个月翻一番，这远远快于CPU性能平均每18个月翻一番的增长速度(摩尔定律)，并且这一趋势有望持续下去。
作者认为，GPU集群对于数据密集型科学计算是有希望的，并且可以在同等成本下大大优于CPU集群
这篇文章的作者是第一个为高性能科学计算和大规模模拟开发可扩展GPU集群的人，建立了一个有32个计算节点的集群，通过1千兆以太网交换机连接。

作为一个示例应用，作者模拟了纽约市时代广场地区空气污染物的扩散。
作者解释了GPU如何用于非图形计算，介绍了用于模拟气体和流体的LBM模型，并展示了如何将LBM模型扩展到GPU集群上
他认为，将LBM计算扩展到GPU集群的主要挑战是最小化通信成本——网络通信和GPU与PC内存之间传输数据所花费的时间。
实验中结果表明:(1)在一个节点向另一个节点发送数据的过程中，如果第三个节点试图向其中任何一个节点发送数据，则中断将破坏数据的平滑传输，并可能显著降低性能
(2)假设总通信数据量相同，每个节点向更多邻居传输数据的模拟比每个节点向更少邻居传输数据需要消耗更多的通信时间。
在这个问题中，作者针对性的设计了通信时间表来优化这些问题。

他介绍到，常规超级计算机或集群需要花费几个小时来求解一个1.6公里×1.5公里的区域，网格间距为10米
相比之下，作者的方法包含了更详细的城市模型，可以在不到20分钟的时间内以3.8米的网格间距模拟纽约市时代广场区域。

作者总结道，由于网络限制，在集群的网络节点数增加时，GPU集群相对于CPU集群的优势逐渐减小
许多类型的计算已经移植到GPU上，但其局限性在于无法有效地处理复杂的数据结构和复杂的控制流。解决这个问题的一种方法是让GPU和CPU一起工作，各自做自己最擅长的工作。



						Hybrid CUDA, OpenMP, and MPI parallel programming on multicore GPU clusters
2011，200cite

NVIDIA的CUDA是一种通用的可扩展并行编程模型，用于编写高度并行的应用程序。它提供了几个关键的抽象——线程块的层次结构、共享内存和屏障同步。
作者提出了一种解决方案，既简化了传统通用应用中硬件加速的使用，又保持了应用程序代码的可移植性，提出了一种使用混合CUDA, OpenMP和MPI编程的并行编程方法
并提出了一种使用性能函数来估计每个节点的性能权重的通用方法，构建了一个混合CUDA集群进行验证。实验结果表明，在混合CUDA集群环境下，所提出的方法比以往所有方案的性能都有所提高。
CUDA	由NVIDIA开发的并行计算架构，NVIDIA图形处理单元或gpu中的计算引擎，软件开发人员可以通过行业标准编程语言访问它
	这些抽象提供两者粒度的并行性，引导程序员将问题划分为可以独立并行解决的粗子问题，然后划分为可以并行协作解决的细子问题。


OpenMP	一种共享内存架构API，提供多线程能力。是共享内存并行性的开放规范。通过使用OpenMP，可以编写能够在多核心、多处理器计算机上高效运行的并行程序。


MPI	消息传递接口(MPI)是消息传递操作的规范。它将每个工作者定义为一个进程。MPI是目前在分布式内存架构上开发高性能计算应用程序的事实上的标准。
	提供了可移植性、标准化、性能和功能，并包括点对点消息传递和全局操作
	MPI库经常用于集群系统中的并行编程，因为它是一种消息传递编程语言。并不是最适合多核计算机的编程语言
	
	OpenMP： 线程级（并行粒度）、共享存储、可扩展性差，适用于多处理器
	MPI： 进程级、分布式存储、可扩展性好，适用于集群
						
						Cluster, grid and cloud computing: A detailed comparison
2011，306cite
	人们需要小规模和低成本的高性能计算（HPC），导致了集群计算的出现。

	网格计算起源于20世纪90年代中期的学术界，其目的是在本地计算中心繁忙时，方便用户远程利用闲置计算能力。
	经过多年的发展，网格成为在动态多机构的虚拟组织中协调资源共享和解决问题的有效途径。
	
	云计算是一种计算模式，大约在2007年底出现。它提供了一个用户可以通过Internet访问的计算资源池。
	云计算的基本原理是将计算从本地计算机转移到网络中，这使得企业在使用所需资源时，无需在购买维护等方面投入巨资，消除了过度供应并提高了资源利用率


集群计算
	集群计算是为了取代超级计算机提出的替代方式，集群主要用于高可用性、负载平衡和计算目的
	高可用性指维护了冗余的节点，负载均衡指用户的需求接收并分发到所有独立的计算机，使得不同机器之间的计算工作平衡，集群通常主要用于计算目的，而不是处理基于io的活动
网格计算
	网格计算将来自多个管理域的计算机组合在一起，以达到共同的目标，解决单个任务
	一种并行分布式系统，可以根据需求，在运行时动态地共享、选择和聚合地理上分布的自治资源
云计算
	既指在互联网上作为服务交付的应用程序，也指在数据中心提供这些服务的硬件和系统软件
	云是一种并行和分布式系统，由一组相互连接的虚拟化计算机组成，这些计算机基于服务水平协议被动态地提供并呈现为一个或多个统一的计算资源。
	可以通过服务提供商交互来快速配置和发布

集群计算的挑战
	中间件:生成软件环境，提供单一系统映像的假象，而不是独立计算机的集合
	程序:运行在集群上的应用程序必须明确编写，其中包含节点之间的任务划分，并且应该注意节点之间的通信。
	弹性:当服务请求数量急剧变化时，实时响应时间的变化
	可扩展性:满足资源的额外需求，从而影响系统的性能
集群计算的应用
	天气研究与预报，使用Hadoop集群来分析客户搜索模式，以进行定向广告，另外包括过滤和索引网页列表等
	同时还用于解决诸如汽车碰撞模拟，数据挖掘、空气动力学和天体物理学等重大具有挑战性的应用。
	集群被用作涉及计算和数据密集型操作的数据挖掘应用程序的平台。它们也用于商业应用程序，如银行部门，以实现高可用性和备份