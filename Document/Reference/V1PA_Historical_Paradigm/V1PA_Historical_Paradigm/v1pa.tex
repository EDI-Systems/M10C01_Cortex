\documentclass[a4paper,twoside]{scrbook}
\begin{document}
\title{Volume 1, Part I: Early Mainframe}
\author{EDI}
\frontmatter
\maketitle
\tableofcontents
\mainmatter

\chapter{Introduction}

what main frames are there?

what's their composition, particularly those that have multiple memory or vector units?


What kinds of memory model research have we done. and what have we learned?

把引用过这篇文章的文章全读了然后把综述写在下面。
Memory Coherence in Shared Virtual Memory Systems
去DOI找，大概至少有500篇左右。参考文献也要做好。另外用英语写吧。


综述：机架级缓存一致性计算机

在这之前，BBN butterfly、Hypercube等计算机已经具备了基于共享内存或通信的并行性，而且有人已经做了小范围的，基于板载总线的多CPU一致性。但没人关心过机架级的一致性问题；大家希望用软件方法绕过它。

机架级缓存一致性计算机是由Li等在1986年提出的（发表于SPDC， Memory Coherence in Shared Virtual Memory Systems）。他们设计了两种算法用以使多台物理独立的计算机共享各自的存储器并运行在一整个地址空间，且各个计算机之间使用Apollo LAN局域网连接起来。这样，数据和进程可以在不同的CPU之间迁移，并且不需要任何的远程过程调用。他们认识到，一个重要的麻烦是内存一致性问题；这个问题和多CPU机器的缓存一致性类似，都比较麻烦。在这个问题中有两个核心点，一个是内存一致性的粒度，另一个则是缓存一致性的策略。他们观察到，在互联网系统中，送大的数据包和小的相比花费的时间差不多；但是，这样做会使得假共享变得严重，因为这种机会大大增加了。他们认为，一种比较好的方法是用一页作为一个单位；这样可以充分利用CPU的缺页中断机制。另外，就像缓存一样，在更新策略上有两种基本的办法，一种是删除其他拷贝（失效），另一种是广播内容到其它拷贝（写回）。很明显，写回算法无法应对大量机器互联的场景。在页面主人的问题上，如果我们把每个页面固定到一个CPU，那么其它CPU上所有对它的写入都要产生一个缺页中断，显然开销也很大。而且，如果某些CPU就是需要重复写某个页面，而且它又不是那个主人，那么就会成为一种病态情况。因此，页面的主人必须是能变动的，让任何CPU都有机会成为主人。另外，中心化的页面主人追踪算法也有这个问题，因此页面主人只能以非中心化的形式追踪。这样，我们就得到了教科书上的基于目录的一致性算法。但这种算法也有问题：我们总是可以找出一种病态情况让它和中心化的算法一样糟糕，比如让所有处理器都去访问另外一个处理器持有的内存，此时它会退化到中心化算法。广播是不可行的，因为这会退化到MSI嗅探算法。然而，完全动态化也不怎么可行，因为这要求每个处理器都存储一份每个内存的主人信息，这导致O（mn）的空间膨胀。在文章的实验中，作者发现，单纯的本地计算任务都非常好，诸如并行独立事件模拟、矩阵乘法等等，但排序这种做大量交换的算法，即便用递归归并排序都显得很垃圾。DSM vs RPC sync rpcs... 


这项工作在1990年很快得到重视。

Fujimoto等（Parallel discrete event simulation）将它用于PDE模拟，并说这相对于软件或通信维持的一致性而言更有益处，因为程序员不要管这档子事了。

Mark.H等一些做大型机器的人（Cache considerations for multiprocessor programmers
）也注意到这个工作，并且指出，即便在这种架构上，尽量让每个CPU忙自己的，避免那些不必要的通信也是很有用的。

Brett.D 等（Fault-tolerance support in distributed systems）在这个基础上第一次提出了DSM，也即分布式共享内存。他拿10MbE（天啊）和PS/2做了一个能起AIX的东西。他注意到，在这种架构中，可靠性也极端重要，而且认识到我们不能总是直接去实现一个容错系统，而是要兼容已有系统，并且不能要求系统本身就是容错的。

John.K等（Adaptive software cache management for distributed shared memory architectures）认识到，对不同类型的数据对象，使用不同的缓存算法，就能提高性能。尤其是，这些缓存算法都是软件实现的，那么多实现几个也不会增加边际成本。具体的分类是（1）只读，（2）频繁写，（3）生产者-消费者，（4）私有，（5）阶段型，（6）取结果，（7）读多写少，（8）同步，（9）其它。对于（1）（4）（7），好解决；对于（2），可以做懒更新；对于（3），我们可以猜测哪个节点将使用这数据，并提前将数据送去；对于（6），可以存一份然后反复更新它，对于（8），可以采用分布式锁技术。May pair with dynamic performance counters, and check the type of that memory. So, based on performance data, extract the patterns that 

同年，Stumm等（Fault Tolerant Distributed Shared Memory Algorithms）提出了能够容忍一个物理机故障的DSM模型。该模型使用复制状态的方法，将关键的数据复制到每一个处理器上。对于最关键的迁移模型，该文章的做法是，（1）当任何一个块被从A迁移到B时，A上仍留有一份拷贝，但被标记为无效，（2）当任何迁移发生时，现时A中所有的脏块都要迁移给B。（2）是专门针对顺序一致性提出的，如果不需要顺序一致性，则不需要（2）。值得注意的是，一旦有容错，而且写比较多，算法的性能会很快恶化到不考虑容错的算法的一倍。

在一个总结文献上，Michael等（Algorithms Implementing Distributed Shared Memory）系统地分析了DSM的四种模式，分别为（1）中心服务器模式，（2）迁移模式，（3）读复制模式，（4）全复制模式，（4）又可以分为（a）基于写失效的和（b）基于写复制的。

StarLite（找不到参考文献了）第一个考虑了DSM的实时性。同年的Axon（Axon network virtual storage for high performance distributed applications，Axon: a high speed communication architecture for distributed applications）则大概是第一个提出要用专有1Gbps互联来搞DSM的（其他人还在搞10MbE），并且直接扩展了处理器架构来支持这一点。

Philip等人（Slow memory: weakening consistency to enhance concurrency in distributed shared memories）注意到，在DSM中只要削减了内存一致性，就可以增加一点并行性。尤其是，如果能用弱一致性语义的话。这是一个非常自然的扩展，尤其是在网络本来就慢的前提下。1988年的PRAM当然也做到这一点，但它和TSO实际上是一样的，不允许读写、写写和读读乱序，做不到针对内存地址的弱一致性。“PRAM: all processors must agree on the order of all observed writes by a single processor; SM: all processors agree on the order of observed writes to each location by a single processor.”SM则允许写写乱序。在今天，TSO都被认为是很强的一致性模型，因为它要求一个平直的、欧氏的通信空间。如果通信空间是扭曲的，那么TSO就难以满足了，但Release Consistency肯定能满足，因为它针对的是物理地址，而任何一个物理地址的修改顺序只能是天然一致的。

除了抽象成大机之外，Tony等人（The design and implementation of a reliable distributed operating system-ROSE）决定，也许实现一个操作系统来直接容忍错误更好。他把RAM复制了，并且对应用程序使用检查点方法（当然也可以在两个物理CPU上一起跑），如果挂掉就爬起来继续跑，并且从操作系统层次对这种继续跑提供了支持。

1990年度的研究是一个小小的开始。

1991年度的主要热点是一个叫Munin的软件一致性协议库。John.K等（Implementation and performance of Munin，Munin: Distributed Shared Memory Using Multi-Protocol Release Consistency）实现了多种一致性协议，并且实现了由DASH系统率先提出的释放一致性（release consistency）。在写出口处，不要立即更新写，而是等一段时间再更新它，也即批量写，这对这种高延迟的系统很有益。在写程序的时候，要（1）标注那些需要共享的变量，然后注明它们的访问特性，然后（2）将所有的同步操作都暴露给底层编译器。这就已经很接近今天的ARM等CPU的一致性模型了。这样，直接优化通信还是要有优势，但是这个优势已经不到10%了。而且，这样也有助于消除假共享，因为共享的单位不再是页了。释放一致性的定义是（1）在任何访问进行之前，之前的所有“获得”操作都必须被完成，（2）在“释放”操作进行之前，任何访问都必须完成，（3）所有的“获得”和“释放”操作都必须是顺序执行的。也即[获得 ---- 一般访问 --- 释放]。

Attiya等人（Sequential Consistency versus Linearizability）回顾了顺序一致性和线性（也即原子性）的一些差别，当然顺序一致性放的更宽，它只要求整个系统最后看起来像是某种一致的顺序就好了，原子性则要求在执行中后者必须立马看到前者的改变，也即在执行时候也真的是前后一致的。一个有趣的结论是，如果整个系统有可靠的时钟同步，那么顺序一致性和线性性居然是一样的开销；但在现实的假设下，顺序一致性会表现得更好。

Michael.J等人（Dynamic Node Reconfiguration in a Parallel-Distributed Environment）注意到，那些来来去去的闲散计算资源也是可以利用起来的。在他们那个时代，这主要是指那些Workstation。他们说，将这些小机器的性能组合起来，很快就可以得到超过大型专用主机的性能。而且，在Amber系统上，他们把进程迁移放在用户态而非内核态去执行，并说这样更灵活。但，整个系统仍然是面向对象的，并不通用，且其内存一致性的单位也是对象。虽然这消除了假共享，但势必要求编程环境、编程语言本身去支持该应用程序。总的而言，仍然要求程序员去手动调整一切。

Austin等人（The Design Scalable of an Operating System for a Parallel Computing Engine）通过Wisdom项目在点对点连接的多处理器上构建了操作系统级的并行计算引擎。他们为此重写了操作系统。他们提出，硬件最好要支持虚拟内存，不然迁移任务很麻烦，搞不好要被迫生成PIC。他们使用的Transcomputer没有虚拟内存，因此迁移变得困难。另外，如果能在一个任务开始运行之前就迁移他，那么麻烦会比任务开始之后才迁移它要小得多。

Michael等人（Constraint Handling, Garbage Collection and Execution Model Issues in ElipSys）将并行用于逻辑编程。他们注意到逻辑编程从根本上就是可并行的，因此将DSM作为一种应用进行了针对性开发。底层它仍然是类似于MPI的，但那些消息是自动生成的。值得注意的是，它的调度器会自觉地寻找那些可并行的块，然后将他们分发给Worker以尽量减少不必要的通信。这个工作由Vlahavas等的（OASys: An AND/OR parallel logic programming system）OASys进一步扩展了。当然，除了Prolog这类东西，分支限界法就更好并行了：Eric等（A distributed implementation of the branch and bound search）就开发出了并行版本。

既然RSC可能涉及到数据迁移，那就有一个问题：究竟是把计算迁移向内存，还是把内存迁移向计算？Tony等（Optimal data migration policies in distributed databases）使用了一个马尔可夫链预测模型，来最小化处理+通信的消耗。如果在存储节点处理，成本更低，那就在存储节点处理，否则就不如把数据送去到能力更强大的处理节点。

Jean等打算走另一条路，就是在设计时候就把整个应用程序设计成是分布式的，并且要求编程语言和框架来提供这种功能。这最终导向了CORBA、IDL这一类中间件，和大量的C/S设计。这显然和通用性背道而驰了，把用户绑定在了框架上。

Yuval.T等（Multi-level coherency management for high-performance shared virtual memory multicomputers）注意到，1986那篇原始论文是存在不足的。它的共享单位是页，这样，速度又慢，假共享问题又多，处理器等待时间就可能变得很长，从而得不偿失。这比较麻烦。他们的观察是，在任何时候，进程间被共享的那点内存其实不多（这点到了2020年基本也没怎么变），那么完全可以用倒置页表来追寻他。这个工作提示我们，如果共享的单位可以小一点，比如，能够放进一个MTU里面，Ethernet共享的性能就可以大大增加了。

Mark.P等为Lisp设计了一个并行编程环境。这环境按照Object来追踪共享，并且还有三大特色：（1）提供多用户之间的无缝协作和共享环境，（2）支持永久性环境存储，（3）提供并行处理能力。每个地址空间都是独立永久自在的，背后的编程环境软件，提供了缓存一致性协议。

Mustaque等使用另一种方法来降低延迟。他们结合了释放一致性和内存分页，也即Slow memory的相关工作的一个扩展，进一步证明了因果性内存（Causal memory）在分布式环境下是一个好东西。这篇文章主要是理论性的，它可能为后面的文章打开思路。

Larry.D在那一年做了一个总结（Computer Networks and Distributed Svstems）。总的而言，DSM有三大好处：（1）将物理上分布的资源共享，（2）将计算机联合起来以增加速度，（3）复制实例以提高可靠性。总之，就是经济性、计算力和可靠性三点。他还提到，（1）多机通信本身，（2）局域网LAN，（3）连接多个工作站的网络是三个大的进步。在未来，还会有（1）光互连，（2）高速分布式并行机和（3）泛在互联，尤其是手机。要广泛并行，需要a）解决并行算法问题，b）解决软件机制问题，c）解决软件编程范式问题。不过，在他看来，缓存一致性其实没啥大不了，因为共享变量改得其实很少。要么，我们（1）在共享变量被修改后就发出去，要么（2）等到用的时候再去取，要么（3）用某种方法提醒计算系统该预取了。在编程工具这块，好的编译器是不可或缺的，因为未经优化的通用代码是非常的慢，可能慢出两个数量级那么多。在泛在网络方面，他甚至提到，有一天要让卫星互联网变得廉价。


1992年DSM缓存一致性得到了更多关注。

Harjinder等（Cluster-Based File Replication in Large-Scale Distributed Systems）关注了大型网络中的文件分发问题。这在今天会变成CDN非常关心的一个领域。CDN的两个主要问题是，（1）找到一个有效的文件拷贝，（2）在这些拷贝之间保持一致性。他们发现，虽然在学术领域两个用户之间的共享可能是很少的，但在商业领域却存在着大量用户在一个玩意上协作的可能。主要有如下四点：（1）用户文件共享很常见，比如文档、源代码、库文件等等，这些玩意甚至一天变几次，（2）很多大文件也被共享，（3）数据在地理位置上非常分散，（4）某些时候这种共享会产生拥塞，可能是服务器端的处理能力，或者网络。不过，虽然如此，写共享的情况却真的不多，因此之前的结论仍然站得住脚。另外，在写失效和写复制之间还有一个中间地带，就是半写失效。完全可以送更新去那些经常读写的节点，并且对于那些实际上不怎么关心这文件的节点，令他们失效便是。而且，这甚至还顺便实现了一点容错性（见1990那个文章）。

Pete等人（Lazy Release Consistency for Software Distributed Shared Memory）观察到，就算是Munin，有时候也会比MPI发送太多的信息。这个时候，可以使用懒释放一致性的算法。我们只要保证，在一个锁被获取之前，锁之前发生的一切内存访问都对大家可见就可以了。如果我不要加锁，那根本无所谓前面的内存写入有没有被传播出去。RC只是把内存的更新推迟到释放，而LRC甚至将他推迟到下一次加锁。

Kermarrec等（Ada communication components for distributed and real time applications）探讨了使用Ada进行并行编程的可能。

Mark.D等（Cooperative Shared Memory: Software and Hardware for Scalable Multiprocessors）探讨了给出共享内存提示的一种可能。具体的做法是，在共享变量访问的前后，加上Prefetch、Presend这样的指令，让下面的通信网络预先启动数据操作，这样就可掩蔽延迟。为达这个目的（1）需要一个编程模型告知程序员什么样的操作是昂贵的并且引导他们提高性能，（2）这些提示不会损害性能，因此程序员和编译器可以插入这些提示，（3）硬件要为那些常见的场景优化。总而言之，得到的是一个结合了（1）共享内存语义和（2）性能调优模型的东西。

Michael.J等（Distributed Shared Memory with Versioned Objects）也将编程语言虚拟机作为底层。它的DSM仅仅存储对象，而且对象可以按分片来同步。他们指出（1）按页来分配内存可能和共享的粒度不一样，（2）缓存一致性机制可能会盲目地进行通信，无法得到来自应用程序的提示。但很显然，这种方法将DSM捆绑在了一种编程语言，或者至少是某种虚拟机上。

R.Ananthanarayanan等（Experiences in Integrating: Distributed Shared Memory with Virtual Memory Management）总结了一些经验。这些经验显示，各个方面，比如数据粒度、内存模型、缓存一致性协议和同步等等都很重要。由于每个节点都是事实上独立运行的，独立管理它们的本地资源，因此使用简单的本地策略来构成一个大的系统是必要的。而且，将DSM和虚拟内存结合在一起是非常必要，且完全可以实现的。

Attiya等（A correctness condition for high-performance multiprocessors）进一步结合了多种不同的Memory Model，并且认为可以对不同的变量使用不同的Memory Model。他们精确地定义了Hybrid Consistency。总的而言，混合一致性模型有如下两点：（1）强操作之间是顺序的，（2）如果两个操作之间有一个是强操作，那么操作顺序和执行顺序一致。总之，只要至少有一个强操作，那么操作间就有保证的顺序，否则就没有。很多人说，弱一致性就足够了，实际上并不是这样；因为总有一个线程需要加入其中。实际上，这和现代CPU的Barrier是一回事；提供了Barrier的弱一致性，就等于混合一致性，这并没有什么大不了。

Thomas L.等（Shared Memory vs. Message Passing in Shared-Memory Multiprocessors）承认Message Passing永远具备更好的性能，因为程序员总是可以精准地控制它们；尤其是在处理器技术飞速进步之后，互联的速度慢慢跟不上了，尤其是通信延迟，这导致信息传递的成本相应增加了。处理器的速度甚至快了两个数量级以上，但通信带宽只减少了一个数量级，延迟减少了甚至不到一个数量级。共享内存在处理器之间负载不平衡的时候尤其有用，因为此时局部性大大增强了，大概率有很多工作将在一颗或者少数几颗处理器内部进行，因此根本不必要发起那样多的通信。而此时，基于通信的算法将仍然保留大量不必要的通信操作，反而拖慢了执行速度。总的而言，CPU越多、通信越慢、负载越均衡，通信越占便宜，反之则共享内存越占便宜。也许可以把共享内存和消息传递结合起来，做一些事情。

Henri.B等（ACOMPARATIVE STUDY OF FIVE PARALLEL PROGRAMMING LANGUAGES）比较了五种采取不同策略的并行编程语言。这说明，在编程语言层面我们能试图解决一些问题，但这并不是最关键的。Thierry Le Sergent等（Incremental Multi-threaded Garbage Collection on Virtually Shared Memory Architectures）进一步展示了，可以在这种语言上直接实现分布式垃圾回收。另一方面，Mark.P等的CLIDE的分布式Lisp研究仍在继续（A distributed Lisp programming system: implementation and usage）。

Avraham等（LRU-based replication strategies in a LAN remote caching architecture）研究了基于LRU的远程内存缓存机制。他们发现，经典的LRU在延迟高的Ethernet局域网上表现并不好，但只要做一个小改动，比如减少允许的最大拷贝的数量，就可以了。当然，在广域网上并非如此，因为节点之间的距离是不相等的，而且可以差得很大。

Aurora在（An overview of the AURORA gigabit testbed）1993年也实现了一些发展。它的主要特色是1Gb连接，和具备专用通信协处理器扩展的高速RISC处理器。这看上去像今天的PLX芯片，或者南北桥这种功能。当年，在DSM上，他们的目标就包括了（1）计算的安全性和隐私性，以及（2）将LAN DSM扩展到WAN。这对今天的我们仍然是一个挑战。

Ngo等（On the influence of programming models on shared memory computer performance）人在三个架构迥异的DSM机器上实现了同样的算法。他们的算法都是由消息传递实现的。三种机器分别是Sequent，一个类似于今天的20路机器，另一个Cedar，既有中心存储器又有局部存储器的机器，还有一个Butterfly，它只有局部存储器。他们的结论竟然是，对于NUMA机器，很多时候使用message-passing还快点，哪怕这Message-passing是用共享内存模拟出来的。这是由于，Message-passing确实暴露了更多局部性，只不过这是以消耗程序员的宝贵时间换来的。
H.V.Leong等（Type-specific coherence protocols for distributed shared memory）在Object-based coherence上更进一步，将他们抽象成Data type。这可以看作是Munin的一个发展和理论化。

Attiya（Implementing FIFO queues and stacks: Extended abstract）研究了DSM的FIFO和Stack实现，并且进行了理论分析。结论也是，线性化是非常强的一个一致性要求，可能并非是有必要的。

John.B等（Toward Large-Scale Shared Memory Multiprocessing，Willow: A Scalable Shared-Memory Multiprocessor）将Munin这种软件DSM和Willow这种硬件DSM拿在一起作了比较。Willow从硬件上支持了缓存一致性，并且每个缓存行的缓存策略是由硬件观察到的存取pattern决定的。这和Gabe的观点很像；因此，这观点不是新事情。在Willow中，多层一致性缓存存在，因此能够利用访存的局部性梯度。这个局部性梯度的很有意思。由多个服务器组成的Rack和Willow是类似的，因为内存也有远近，因此那种可供利用的梯度仍是真实存在的。

Tamir等（Hierarchical Coherency Management for Shared Virtual Memory Multicomputers）提出了一种页缓存单位和行缓存单位之间的折衷：按页缓存粒度太粗，按行缓存代价太大，那么仅仅在该页被实际共享的时候才按行缓存该页的内容就可以了。

L.Gunnaseelan等（Distributed Eiffel: a language for programming multi-granular distributed objects on the Clouds operating system）做了一个Distributed Eiffel，也是基于Object共享的编程语言。

Olivier等（Eos, an environment for object-based systems ）做了一个编程语言方向的集大成者Eos，并且试图用它来提供一致性。Alvaro做的EZ（Distributed EZ (string processing language)）也是同一个东西。

总的而言，1992年关注DSM的人多了起来，各种一致性奇技淫巧开始大爆发。大家开始寻找那些优化的不够彻底地地方，放宽约束、寻找折衷。总的而言，思路是，当发现两个极端之间差距过大的时候，试图寻找一个中间地带来弥补它。
















1993年DSM可以说炙手可热。甚至可讲，93年的论文占了半壁江山。当然，1994也是差不多光景。

1993年的开篇巨作是Chandramohan的（Limits to Low-Latency Communication on High-Speed Network），它详细地解释了高速网络里面的限制因素有哪些。他们制作了一个新的RPC系统，它非常快以至于延迟开销基本都在交换机和网线上。从测得的数据看，至少对于以太网而言，绝大多数时间都在网线上，而我们基本对此无能为力。也许，今天的10GbE、100GbE会大大减少网线时间，比如减小到，1514的包。24.42us。从这个角度讲，如果分布式应用程序能够用DPDK外加低延迟网卡，说不定有得玩。如果是RPC的话，操作系统开销也是一个难以忍受的点，不过它麻烦就麻烦在组分很多，都很小，难以优化。另外，他们确认C语言就足够了，没必要用汇编，时间差不显著；今天更是这样，因为网卡驱动反正都是C语言。

Steven.K等（Kernel support for the Wisconsin wind tunnel）在一个无一致性的消息传递机器上实现了一个共享内存系统，并且用它来研究共享内存系统的性能。这机器有一个特点，就是有一个CPU专门负责跑操作系统，另外几个CPU仅仅负责执行线程。这看起来有点像今天的“Federated Scheduling”。

Berkeley很早就搞出了供学生演练用的操作系统Nachos。他是一个运行在Unix上的进程，并且它可以玩DSM这种花活。在当时这可是科研前沿。做一个研究操作系统，其关键的是，要决定哪些自己写，哪些留给学生。通常而言，链表、位图和线程切换不要给学生写。这些太难了。其他部分可以给他们写，然后按照实现质量评分。

Henri等（Object Distribution in Orca using Compile-Time and Run-Time Techniques）实现的Orca系统也是使用共享内存对象来进行的。编译器不仅会选择对象使用的一致性模型，还会选择对象的存储位置。这一切都是自动的，不需要像Munin那样的人工标注。当然，未来甚至可在运行时动态优化。Orca并非一门面向对象的语言，它也仅仅是被设计用来研究并形体语言设计的。总的而言，评估这种系统的性能，可以用TSP，或者APSP问题。当然，今天我们有更多dApp可以用。

Sandhya等（Evaluation of release consistent software distributed shared memory on emerging network technology）评估了当年的网络技术对DSM实现的影响。其结论是，带宽不那么中用，而延迟则是非常要命的。因此，未来的DSM应该尽量减少同步，内存模型越弱越好。不过，对于粒度很粗的应用程序则不是这样，完全不受影响，因为它们其实不怎么同步。具体而言，他们是用仿真来进行的实验。

Ambuj等（Using shared virtual memory for parallel join processing）将共享内存用于数据库并行Join。他提到，可以同时使用消息传递和DSM来提高性能，这两者不矛盾，尤其是在那些原生的Shared-nothing的系统里面。

Karin等（Cache coherence for shared memory multiprocessors based on virtual memory support）居然试图在那些完全可以做片上互联或者高速总线snoop的场合使用基于页表软件的一致性：这通常是以太网才会用的方案。结果是灾难性的；在轻度争用时性能尚可和snoop一战，一旦越过4核或者高度争用，性能就不到snoop的一半了。这再次说明，哪怕在那些总线速度快的前提下，基于页表的软件实现也是快不过硬件，没戏。

Avraham等（Replication Algorithms in a Remote Caching Architecture）指出，在分布式系统中，对象缓存要考虑如下四点：（1）哪些是热点，（2）可用的缓存有多大，（3）不同的节点之间的访问方式之间的差距有多大，（4）最优性能的标准，是带宽，延迟，总处理能力，节点间有无均分负担，还是什么别的玩意，比如实时性？（当然他们还未意识到实时性是非常重要的一方面）。

Michael等（How to find his way in the jungle of consistency criteria for distributed shared memories）发现，弱一致性，或说混合一致性，确实是最好的解决方案。他们横向对比了很多东西。

Shigekazu等（Using Huge Address Spaces to Construct Cooperative Systems）使用64位页表中的那些未使用的位来标记可能存在虚拟地址冲突的那些共享数据结构的分区，并且使用缺页中断来动态映射这些数据结构。如果两个要被共享到同一个空间的的数据结构虚拟地址存在冲突，而它们内部又可能含有指针而使我们无法移动他，那么这个时候我们就必须不断切换该地址处映射的数据结构使我们能做正确的访问。为此，那些页表高位被用作数据结构间的指针。我们无法更新这些指针（他们在数据结构里而且我们不知道他们在哪），因此要保证按照之前的所有版本的指针都能访问正确的数据结构。这就需要将一个数据结构在不同地址映射多次。或者说，当冲突发生时候，在地址前面加位来区分它们。这显然太麻烦了，而且需要操作系统级别的支持，且会浪费大量的地址空间。最后，留下来的解决办法是位置无关代码（PIC），或位置无关数据结构。

WADA等（A PERFORMANCE EVALUATION OF TREE-BASED COHERENT DISTRIBUTED SHARED MEMORY ）实现了一个基于树状目录一致性的DSM。他们说这个东西对于矩阵乘法，32个CPU能快21倍。可以看到，节点的延迟高了，性能是成倍下降，一倍延迟就是一半性能。

John.C等（Network Multicomputing Using Recoverable Distributed Shared Memory）试图在DSM上实现容错。使用的方法是老两样，一个是复制，一个是存档回滚。但，这种能力能够施加给已有的编程模型和系统吗？

Lumetta等（Decentralized Optimal Power Pricing: The Development of a Parallel Program）针对并行计算机集群编写了电网优化程序。在程序编写的过程中，他们手动调整了最优化的方向，并且证明64台计算机可以加速48倍运算。这说明，专用的MPI式编程方法确实是有道理的。很多时候，一个问题的并行版本和串行版本写法并不太一样，因此去探测串行版本中的并行性有时候还不如重写。

Bemd Irlenbusch等（Type Specific Fault Tolerating Protocols for a Distributed Object Memory）很快意识到，不仅仅一致性协议可以按照对象类型来分，连容错方案也是这样。不同类型的对象完全可以采取不同的容错办法。

惠普公司（Data Merging for Shared Memory Multiprocessors）在1993年那时便知道可以推迟一致性通信直到加锁的位置，而且知道把内存放在一起的好处。他们要找的是如下东西：（1）任何一个处理器硬件不需要知道共享的数据在多个处理器那边都缓存了，当然（4）程序员和编译器也不需要知道；（2）能够对假共享问题给出一个满意的效率；（3）该模型可以利用弱内存模型允许的额外宽裕时间；（5）解决方案适用于多种内存数据粒度；（6）解决方案无论是软件实现还是硬件实现都很高效。在这个解决方案中，他们把内存集中起来放，这保证了目录在内存处理器那边专做处理，内存处理器可以为这个事情做优化，无论在带宽和容错性方面均是如此，而且处理器挂掉不会导致内存一致性数据的损失。这就是1993版本的CXL.mem，令人吃惊的是，CXL的目的在三十年后的今天都没有什么变化。

Bob.J等（Relaxing consistency in recoverable distributed shared memory）发现，内存一致性模型和容错不是两个孤立的问题，他们联合起来还有其它好处。比如，一个弱内存模型可以将Checkpoint的时间尽量推迟，比如推迟到锁释放的地方，甚至推迟到下一次加锁之前。

Abdelsalam等（An Overview of Mermera: A System and Formalism for Non-coherent Distributed Parallel Memory）认识到，如果我们干脆在软件层面去容忍这种共享内存间的不一致性，甚至在同一个程序内容许一致或不一致的内存段，那么性能也许也能提高。他们对一致和不一致的内存做了一个统一的形式化描述，并且画了很多有意思的图来详尽展示不同内存模型之间的区别。

Andrew等（Hardware Assist for Distributed Shared Memory）发现，可以在纯硬件和纯软件实现的DSM之间做个折衷，使用硬件来完成那些软件效率不高的操作。而且，不一定要使用写失效，完全可以使用写更新，因为现在一致性单元变大了，写更新反而更有效率，不必要每次都丢掉整个一致性单元，而只要传输那些被更新的字节就好了。他们说，写更新并不是总不好，当你有很大的页面时候，甚至于可能就是最好方案。写更新其实甚至可以被看成是某种message-passing的变体。但写更新可能有个问题，怎么判断它写完了，然后我发送？如果软件实现的话，要发生多少fault handler system call，简直是不可想象。写一个字节就发生一次？难以置信。因此，在这个地方，设计一个集成到MMU里面去的硬件，然后让它探测Update，然后自己把封包发出去就好。实际上，这个地方的Update也可以和Invalidate结合起来，比如多久Invalidate一次，这样让那些没有使用该页面的节点彻底不保存该节点的拷贝就可以了。

Liviu等（Memory Servers for Multicomputers）也想到把内存放在一个机柜里面，当然这样做的理由是“一个机器上插不了那么多内存条”。这个里有在今天不成立，在机器上插满内存条的密度和专用的内存机柜其实差别已经很小了，反正现代子星服务器连PCIe插槽都没有。这个理由和HP的比起来并不那么本质。当然，集中的内存机柜可能会引起二次内存浪费，因为哪怕所有的页面都是Exclusive的，他们也要在各个子节点那边被缓存一遍，而真正的分布式内存则没这回事。


在1993年，一总结文章（Recent trends in experimental operating systems research）讲到，“高性能操作系统”、“分布式共享内存”、“高效同步”、“身份认证”、“分布式文件系统”、“分布式应用程序构建支持”、“高性能通信”、“大位宽CPU”是当年的几个热点。而且，系统研究通常有几个特点：（1）技术进步会催生新的系统研究。（2）我们要找出并为那些常见情况优化。（3）微电子、CPU、OS、编译器、编程语言是一个连续统，这方面的通才是最重要的。（4）最好的工作为那些重要的问题提供解决方案。很容易做那些边角料工作：不重要的问题，或者不怎么样的方案。

另一篇Tannenbaum的总结文章（Distributed operating systems anno 1992. What have we learned so far?）说，（1）分布式系统必须是基于微内核的，（2）C/S架构还不错，（3）Unix可以跑在用户态，（4）RPC用来做通讯很有效，（5）全局有序可靠广播很有用，（6）通讯透明化很重要，（7）DSM确实会让编程变简单，（8）最重要的，他反对提供很多种机制给用户让用户去选择，而是要求操作系统来处理这些事情。

总之，1993年开始，就有人能够将DSM做的相当深入了。很多重要细节得到讨论，并且折衷方案开始大量被提出。这里面，有软件实现-硬件实现的折衷，有集中-分散的折衷，有写失效-写拷贝的折衷，有强一致性-弱一致性的折衷，手动优化-自动优化的折衷，共享内存-通信的折衷。到这里为止，对DSM的限制性因素或者说设计它所要考虑的因素，大家都很明白了。


1994年研究和1993年差不多火。但很明显，研究开始更加深入了，甚至开始为每种（每个！）应用程序定制属于自己的一致性协议或者内存模型；开始研究无线网络下的DSM；开始研究自演化一致性模型；用更小的页面单位来维持缓存一致性；针对DSM的特点来实现算法；

Michael等（Integrating Coherency and Recoverability in Distributed Systems）发现，缓存一致性和容错可以结合起来。只要将每次对存储器的更新都形成日志，然后把日志放在某个地方就好了。等日志都存好了，再提交。当然，它们实现的这个东西更接近数据库的要求，对并行程序来说，原子性的要求太严苛了。

Vincent等（Distributed filaments）意识到，我们应该将应用程序表述成它内部允许的最大并行度所允许的样子，将它拆得尽量细，而非反过来让应用程序的代码去适应硬件架构。这样就等于最大限度地暴露了应用程序本身的并行性。虽然这在CPU少的机器上可能不怎么性能好，但是在那些CPU多的机器上，应用程序不用重写就有好性能。更重要的是，这样做原则上可以暴露全部的并行性，而不是需要对每种硬件再去调优。尤其是在负载均衡上，会非常好。

Matthow等（Software Write Detection for a Distributed Shared Memory）使用软件方法来探测对DSM的写操作。这毫无疑问是可行的，但这就有两个坏处：1.编译器要修改，2.库文件也要修改，最后还不如做成Munin那种纯软件方案。相比之下，1993年那个修改硬件的工作还要好一点。

Netware等操作系统（An Overview of the NetWare Operating System）已经具备和Composite一样的RPC功能。这意味着也许它有更好的容错能力。他们意识到，这种RPC确实非常有效，因为它中间用什么介质对应用程序来说是完全透明的。

Mustaque等（Causal Memory Meets tile Consistency and Performance Needs of Distributed Applications!）认为因果内存就足够好了，这很有意思。

James等（LCM: Memory System Support for Parallel Language Implementation）做了一门语言叫C**。这个语言的特点是，在C++基础上扩展了SIMD并行语法，并且这个语法和今天的CUDA的很相似。

Chandramohan等（Hardware and Software Support for Efficient Exception Handling）讨论了一种1990年设计的Tera架构，这个架构有两点有意思的地方：第一点，它的SMT是128SMT滚筒式执行，大大提高了执行部件的利用率，但是要求将线程切的非常碎。这个处理器还具备用户态中断执行等功能。当然了，这个架构并不包含时间域的保护，这意味着中断想干什么就干什么。SMT最大的弱点就是，它虽然可以减少单个线程占用的执行部件，但是无法减少单个线程的存储部件，甚至于激化了这个矛盾。而在低功耗领域，直接顺序执行不要SMT反而是最优的，因为SMT主要是用来提高超标量处理器的部件利用率的。家用领域，2SMT基本上最优的，服务器则2SMT和4SMT差不多。至于8SMT，在什么时候都不是最优的。

进一步，Chandramohan等（Separating Data and Control Transfer in Distributed Operating Systems）意识到将数据流和控制流分开是很有好处的，因为RPC是同步通信模型，而且同时转移控制流和数据流，且需要将参数打包解包，效率很低。而且，由于控制流总是要转移，服务器端的调度器总要参与，在这整个过程中客户端必须等待，这进一步导致了性能低下。因此，完全可将数据流提出来单独处置，这样就可以异步操作了。

Steven等（Tempest and typhoon ）提出，可将共享内存与通信混用，并在用户态暴露接口给用户。但总的而言，一旦它这样做了，他就更接近一个消息传递系统。降低软件开销的一个方法是使用专用的网络处理器。这类似于用DPDK+Smart NIC实现DSM。

Michael等（Fine-grained sharing in a page server）和Andreas等（Using subpages for coherency control in parallel database systems）提出，在共享内存系统，比如数据库中，若只使用一种页面共享粒度，就无法适应所有情况。这个时候，可以对绝大多数情况用按页的共享粒度，另外一些情况则使用按对象的共享粒度。这和那篇可变缓存行大小的思路是一致的，有助于尽量减小网络通信负担。但它无法掩盖通信延迟本身。不过，如果考虑节约通信流量的话，这倒是最好的。当然，Andreas则是使用了subpage，按理说是不如object的。

Chandra等（Petiormance Evaluation of Hybrid Hardware and Software Distributed Shared Memory Protocols）则认为，细粒度的共享可以交给硬件，粗粒度的则可以交给软件，这是因为细粒度共享快且假共享少，但会增加很多不必要的开销，且只能在处理器缓存中存储数据，而粗粒度共享则虽然慢且假共享多，但可以在主存中进行数据缓存。另外，可以使用entry-based consistency，将同步原语和它们所要保护的数据区域结合起来，这样那些数据区域之外的数据就可以不受该原语影响了。不过，这需要程序员提供额外信息才可以。

Yixiu等（Data Replication for Mobile Computers）研究了在计费流量连接的场景下最小化共享数据结构通信开销的方法。他们建立了一个模型，来最小化通信量，因为数据服务提供商是按照流量算钱的。

Hsiao等（A distributed shared memory system with self-adjusting coherence scheme）扩展了Munin的概念，但不是依赖于程序员的标注，而是自动探测合适的缓存一致性协议。他们把所有的变量简化成了三种，分别是总是读、多人写、频繁写三类，并针对它们采用不同的一致性协议。

Alvaro等（Garbage Collection IN DISTRIBUTED EZ）提出了对分布式对象编程语言虚拟机的一种内存垃圾回收算法。

Lampson等（Interconnecting Computers: Architecture, Technology, and Economics）指出，现在的计算机网络架构的特点在很长一段时间都肯定不会变化。毫无疑问，从三十年后的今天看回去，这个结论是正确的。

Hagersten等（Simple COMA node implementations）提供了一种激进的设想，COMA，的一种具体实现。COMA是指每个处理器的虚拟存储器全部被用来作为缓存，从实地址到物理地址之间还有一层转换，而这层转换在传统的共享内存架构中是没有的，每个物理内存块所在的节点是固定死的。这对于那种需要缓存-内存分配的灵活性的场合，比如可能物联网，非常有用。

Larry等（Dynamic snooping in a fault-tolerant distributed shared memory）设计了一种可以容错的DSM一致性协议。总的而言，每次节点进行写提交从而变成该页的Owner时，如果其它节点从该节点读取数据，则读取之前所有的脏页会被先原子性地发给snooper来保存一个副本。也即，snooper保留了该节点向外发送的所有的内存的一个副本，在该节点挂掉时候可以代替他响应请求。那为什么不在该节点写内存的时候就通知给snooper知道呢？因为没必要，如果状态没传递出去的话，那就和不存在一样，我们可以把这种情况等效为机器在一开始就坏掉了。

Bob等（Reducing Interprocessor Dependence in Recoverable Distributed Shared Memory）注意到，在DSM系统中，没有理由像Message-Passing系统一样在Checkpoint回滚所有的Message。这是因为，在DSM中，并非所有的信息都携带有依赖性；只有那些写操作才携带依赖性。这和Message-Passing系统不同，Message-Passing系统的信息都可能产生潜在依赖。Janakiraman的（Coordinated Checkpointing-Rollback Error Recovery for Distributed Shared Memory Multicomputers）也是同样的思路。

Babak等（Application-Specific Protocols for User-Level Shared Memory）进一步发展了Tempest。它在Munin的横向分类上更进一步，为每个应用程序纵向定制了DSM一致性协议。它和Munin有一个重大不同：Munin仅允许程序中的每个变量在系统提供的几种策略里面选择一个，而Tempest则允许每个应用程序的策略完全自己定制（而不仅仅是选择），定制化程度更高了。当然，这也让他更接近一个message-passing系统。

Yixiu等（Divergence Caching in Client-Server Architectures）发现，在某些数据库中，并非每次都要求读到最新的数据。很多时候老数据也是可以容忍的，这个时候没必要那么着急发起通信。这可以省掉大量的通信费用。

Lee等（Concord: Re-Thinking the Division of Labor in a Distributed Shared Memory System）将DSM中维护一致性的职责分配给程序员、编译器和运行时。这和传统DSM将这些功能集中在一个层次上完成一致性不同。它有Folder的概念，Folder是共享的一个最小单元，把Object放进Folder就好了。

Cox等（Software Versus Hardware Shared-Memory Implementation: A Case Study）对DSM的软硬件实现做了更深入的测试。他发现，全软件实现的扩展性确实是非常不好，全硬件实现的扩展性确实是不错，而软硬件联合的扩展性（也即，软件负责8路以上的节点，而每个8路以内则交给硬件）看起来比较合适。这样，硬件不会过于昂贵，软件上也算说得过去。也就是说，在构建DSM系统时候，应该购买硬件路数最多的机器，然后在这个基础上部署软件互联。

Tiran等（A Generic Multi Virtual Machines Architecture for Distributed Parallel Operating Systems Design）开发了一个操作系统内核，它能够支持多种上层虚拟机，然后让每个虚拟机为每种编程范式提供相应的支持。

Thorsten等（Low-Latency Communication Over ATM Networks Using Active Messages）认为，可以使用低时延的ATM网络代替普通的以太网能够增进通讯效率。他们对比了传统以太网实现和CPU内部的总线实现，发现CPU内部交换网络快至少两个数量级。也即，多核CPU与一堆网络互连起来的单核CPU是有相当程度区别的，除非那些网络也能一样快。而且，除非把缓存一致性的PHY整合到CPU里面，否则是没戏的，延迟降不下来。ATM能使用电路交换、分组交换，尤其前者是低延迟的关键之一。电路交换在片上实现十分简单，可以说片上最便宜的就是导线了；但片下就不一致。

ATM网络失败的教训：Lessons learned? First, do not count on supporting “a little inefficiency” while the ecosystem catches up to a big new idea. Either the system has immediate, measurable benefits, or it does not. If it does not, it is doomed from the first day of deployment. Second, do not try to solve all the problems in the world at once. Build simple, use it, and then build it better over time. While we all hate being beta testers, sometimes real-world beta testing is the only way to know what the world really wants or needs. Third, up-and-to-the-right charts are easy to justify and draw. They are beautiful and impressive on glossy magazine pages, and in flashy presentations. But they should always be considered carefully. The underlying technology, and how it matches real-world needs, are more important than any amount of forecasting and hype.

真正有趣的是这个文章：中国国内的研究者，如孙钟秀（1936-2013）等，在非常糟糕的国内科研环境下开始向国际接轨。一篇中国期刊文章（Adaptive Memory Coherence Algorithms in DSVM）评估了国外的各种一致性协议。它1994年发表在计算机科学技术学报（Journal of Computer Science and Technology）上面。今天的孙钟秀实验室叫做“南京大学分布式计算实验室”。


Kermarrec等（Integrating Page Replacement in a Distributed Shared Virtual Memory）发现，DSVM中的页替换也可以和缓存一致性协议结合起来。

Glasco等（UPDATE-BASED CACHE COHERENCE PROTOCOLS FOR SCALABLE SHARED-MEMORY MULTIPROCESSORS）发现，基于写更新的缓存一致性协议并非不好，因为写操作可以被合并起来（用一个简单的写缓冲区就可以做到，比如x86的TSO实现那样），而且同步的粒度也可以更细一些，这样就能够提升基于写更新的一致性协议的性能。当然，它们的研究是基于多核CPU做的，但对于多机DSM未必就不行。

总之，1994年结合性的工作做得更多了。大多数都是缓存一致性协议和XX的结合，这个XX可以包括容错、页面替换、软件-软硬结合-纯硬件之间的权衡、更深度定制一致性协议，达到每个应用程序搞自己的一套一致性协议的程度（这和消息传递已经区别很少）。这也说明，DSM领域的低垂果实基本都被摘完了，做无可做。


1995年做的非常杂，绝大多数都是DSM和XX的结合，而且还是那种没用的XX，比如垃圾回收、信号量、容错（这个都做了多少工作了？）、页面交换、数据库、数值仿真算法，等等。在这一年，有人又开始怀疑多机DSM是否真的那样有用。有人在这一年也做了一些总结性的综述，试图把前几年的工作综合起来。

Henry等（Higher-Order Distributed Objects）将负载均衡和线程迁移挪到了用户态并且试图让用户指定policy。这样，可以提高一些效率。这毫无疑问可行，但是代价也是很大的。

Bennett等（Techniques for Reducing Consistency-Related Communication in Distributed Shared-Memory Systems）在这一年进一步优化了Munin。现在，不仅仅是使用Write-invalidate协议，Write-update协议也可以使用了。另外，完全可以将很多个写操作推迟到release的时候一起发布，也即buffered，而非pipelined。

Saulsbury等（An argument for simple COMA）进一步分析了COMA的概念。它们将原先的COMA做了简化，而且这次似乎能够使用含有MMU的货架产品来搭建它，除了DRAM需要有一个Hash tag之外。不过，为了这个Hash Tag，仍然需要设计客制化的、针对DRAM的缓存控制器，这仍然会成为一个问题。在这个设计中，软件和MMU负责存储分配，而硬件缓存控制器负责一致性，这样缓存一致性的单位（缓存行）和存储分配的单位（页）就可以不一样，大大减少了开销。甚至可以说，COMA就是具备大缓存的CC-NUMA，或者硬件负责实现一致性的DSVM。不过，从今天看回去，CC-NUMA确实是取得了最终胜利。但是，在网络DSM这边就未必了。COMA显然能够通过更大的缓存掩蔽掉更多延迟，因为现在延迟变得非常大。

Feeley等（Implementing Global Memory Management in a Workstation Cluster）在工作站集合中打造了一种能够从全局管理存储的方法。这很类似Linux的超然内存，仅仅是将其他机器上的内存用于I/O缓冲、虚拟机分页、文件映射等等。就凭这点，很多时候对内存型工作负载也有1.5倍到3.5倍的提升，而根本不需要什么DSM。不过，可以想见，对于内存型工作负载，DSM可能确实会很有用。之前我们总是考虑计算型负载和通信型负载，并且评论说DSM如何如何不合适，但对内存型负载，DSM会非常合适。

Cierniak等（Unifying Data and Control Transformations for Distributed Shared-Memory Machines）从编译器优化的角度试图应付DSM效率低下的问题。他们指出，一个程序的绝大多数时间都花在循环上，如果能够很好地利用代码变换（比如LICM、LR、Skewing、Reversal、Tiling等）和数据变换（如更换行列位置等），代码和数据的局部性会进一步增加，此时若能在这些变换中考虑DSM架构本身特点（如假共享就是多等等），并将代码与数据变换结合，就可以大大提升性能。这并不意外，因为一个好的编译器总能针对微架构进行优化，而对DSM而言，它的实际的通信网络的组织形式就是其微架构。

Oner等（The Design of RPM: An FPGA-based Multiprocessor Emulator）注意到FPGA可以用来做DSM系统的设计和模拟。这在今天当然是很简单的一个想法，但30年前的FPGA还是很幼稚的。它们的规模及其有限，基本不存在任何SoC可能性，就连实现一个Cache控制器都费劲。因此，他们被迫使用了多个FPGA，并做兼容设计以兼容未来的FPGA。

Tannenbaum这个微内核拥趸（THE AMOEBA DISTRIBUTED OPERATING SYSTEM—A STATUS REPORT）指出，微内核才是分布式系统的最佳出路。这是因为传统的两种方式（1）在主机上运行全功能操作系统，从机仅做裸机和（2）所有机器均运行全功能操作系统无法（1）在多用户情况下共享超级计算机（2）进行时间和空间上都高效的通信。在那个时代，这些微内核也是“Capability-based”，但它们的Capability更像一种口令：一旦给出，无法撤销，且相当容易被伪造。这看起来其实是返回了一个Constrained Resource ID，而不是一个Capability。虽然可以加入检查域防止伪造，但检查本身就会消耗时间。而且，这些Capability都是Server-Side的，在Client-Side想怎么传播都可以，实际上根本缺乏权限控制。这些Capability仅仅能够控制资源的引用权限，但无法控制不受信任的应用程序传播这些引用权限。

Baraloo等（CALYPSO: A Novel Software System for Fault-Tolerant Parallel Processing on Distributed Platforms）意识到，可以在顺序程序中插入关键字来并行化它的某些部分。这样，他们在1995年便得到了一个非常类似OpenMP（1997年才被发明）的语言。它们的程序也是针对多机的，这和现在的OpenMP不一样。

Protic等（A Survey of Distributed Shared Memory Systems ）意识到现在这个领域已经有点乱了，新人不好把握，于是他做了一个综述。总的而言，看起来像下表。在综述的结尾，他写道，DSM一定是未来，因为这可以大大降低并行程序的开发难度，等等，而且说商业实现一定会出现并且成功的。当然现在我们知道，他过于乐观了。从下表里面可以总结出这么几点：（1）针对不同的内存用不同的一致性协议，（2）不同的对象可以用不同的原语独立维持一致性，（3）硬件去干那些脏活累活，比如探测更新、一致性维持，软件则负责具体的策略管理，（4）要想办法优化那些常见情况，交给硬件，不常见的情况可以交给软件。另外，如果要纯软件，Munin+Midway基本就到头了。
DSM	分类	年份	学校	备注
Memnet	纯硬件	1991	Delware	硬件拓扑是令牌环网总线，看起来像是SMP结构直接拿出来到多机之间了。可扩展性有问题，因为目录结构是平方空间复杂度。类似于今天的消费级Intel CPU的内部结构。
DASH	纯硬件	1992	Stanford	一个经典的2级树状目录结构，目录也是平方复杂度，只不过这个平方复杂度对应的是小组数量，不是单机数量，一个小组里面可以有多台机器。类似于多插槽的AMD CPU的内部结构。
SCI	标准	1992	IEEE	将目录用链表表示，避免分配固定的空间给它。最大限度减少空间占用。但，此时所有的读操作都要向中心节点发起，这本身就很低效，而且中间节点如果挂掉就是灾难性的。值得一提的是，有一个企业实现了基于PCI/PCIe的SCI，这看起来其实很像CXL。
KSR1	纯硬件	1993	Kendall公司	COMA机器，多级树状目录结构，目录空间的问题确实解决了，但是延迟就很大，而且这个树状结构一经指定就无法更改。树状目录结构能压缩表示的原因是他将多个节点的状态表示合一了。
DDM	纯硬件	1992	SICS	单总线式的snoopy一致性协议。性能好，但扩展性糟糕。
RMS	纯硬件	1992	Encore公司	单总线式的snoopy一致性协议。一旦发生写，就会广播。
Merlin	纯硬件	1990	SNL	静态编码的共享方式。一旦共享内存被更新，那么就会被发送给所有潜在的节点，哪怕那节点并未请求数据。这非常简单，但是非常笨。
Munin	纯软件	1990	Rice	不同的数据结构使用不同的缓存一致性策略。编译器插入代码来维护一致性。
IVY	纯软件	1989	Yale	按页维护一致性，并采用一个中心化的DSM Manager。当然，也许也可以使用分布式Manager手段。
Linda	纯软件	1986	Yale	按对象维持一致性，且对象一经创建便为只读。要想写对象，必须创建新对象并销毁旧对象，而且广播这个创建。一个奇怪的点是，可以根据对象的内容来查找对象，这让整个实现变成了一个CAM。
Mirage	纯软件	1989	UCLA	和IVY类似的按页一致性，但页分的更细（512B），而且采用一个参数来控制页主人的最小转换间隔。这等于实现了Batching，平摊了软件实现的成本。
Clouds	纯软件	1989	GIT	按段组织的对象一致性。不同的段可以有不同属性（类似Munin），但是一个对象可以含有多个段。
Orca	纯软件	1988	VU	共享数据对象模型，使用类似数据库的2步方法来更新所有的对象。这用在DSM里面或多或少有点脱裤子放屁了，因为在DSM中Commit总是可行的，不存在不可行的情况。
Midway	纯软件	1993	CMU	Entry consistency。这个一致性模型将每个对象和一个同步原语关联起来，当该同步原语遭操作时才同步对应的对象，而不是在一个同步原语操作时就更新所有的对象。它把同步和更新切得更细了，当然这样也更复杂了。
Agora	纯软件	1988	CMU	基于对象的一致性，类似Linda，要修改就必须创建新对象，但在这里旧对象是用垃圾回收来回收掉的，并非程序员手动删除的。
Mermaid	纯软件	1990	UT	类似IVY，但它可以在多种处理器构成的异质网络上运行。
PLUS	混合型	1990	CMU	缓存一致性和同步由硬件处理，但页映射和迁移由软件（操作系统）处理。
Alewife	混合型	1991	MIT	一个具备有限的硬件缓存目录的实现。剩下的那些缓存目录放在软件里面。如果硬件不够用，软件就Kick in。一个很聪明的设计。
Lynx	混合型	1992	WPI	硬件负责发送一致性消息，但软件告诉硬件该对谁发送什么样的一致性消息。硬件负责那些工作量很大的一致性通信，软件只是告诉硬件要怎么通信。
Flash	混合型	1994	Stanford	和PLUS类似。
Paradigm	混合型	1991	Stanford	类似KSR1，但其不一致是由硬件探测的，软件则负责缓存管理。

Elnozahy等（The Performance of Consistent Checkpointing）指出，连续检查点并不会导致性能有多少降低，也就1%-5%样子。它的具体机制就是每隔一段时间让所有节点连续执行2步提交算法，执行备份时候整个系统程序仍然运行。

Carter等（Distributed Shared Memory: Where We Are and Where We Should Be Headed）指出，现在的DSM性能对于中等粒度的应用程序已经够用了，但是（1）现有系统基本都是为科学计算定制的，（2）没有免费可得的DSM系统，（3）现有系统和遗留基础软件的结合不佳，（4）没有可用的调试器，（5）对于细粒度任务而言可能还是性能不足。他特别指出，“发明新的机制以提高DSM性能可能是让你获得SOSP或者OSDI文章的方法，但是现在的DSM对于商业应用程序已经很足够了，但我们缺乏成果转化”。这是这一年的一个很好的总结。



1996年
这一年主要都是Checkpointing研究，间或夹杂几个优化，比如Entry Consistency优化、预取优化、垃圾回收优化等等，还有基于双链表的目录结构的优化等等。也有几个综述，看起来还行，但是都是细节。这里就只挑选高被引的主要文章了，其余细节都不管。

Markatos等（Implementation of a Reliable Remote Memory Pager）继承了Frelly去年的工作。他从超然内存的角度出发，主要是增长系统的可靠性。他提出，在页面扇出时直接把页面丢给其它节点就好了，如果要可靠就丢两份出去。如果嫌两份太多，就可以用奇偶校验节省一点空间。

Blumofe等（Dag-Consistent Distributed Shared Memory ）实现了一种叫做DAG consistency的一致性模型。如果把一个计算拆解成DAG，那么DAG中的并行部分其实不需要看到互相的更新，只要后面的串行部分看到前面部分的更新即可。因为并行部分计算完成后会Sync，因此串行部分肯定看得到这个更新。不过，这个所谓DAG consistency其实是暗含在其它形式的Consistency里面的，因为如果DAG并行部分里面的各个线程用它们自己的存储器，那么Consistency根本就不会Kick in（各自的内存会被标记成Exclusive）。这个DAG consistency只有在发生假共享时候也许才有用，而且需要我们分析程序本身的结构。如果缓存一致性的单位足够小，就没必要用这个。

Freeh等（Dynamically Controlling False Sharing in Distributed Shared Memory）引入了一种新的一致性模型，这种一致性模型会试图将原有的、引起了假共享的对象重新映射到不同而定存储单元以消除这种假共享。这需要小于页面大小的映射，或者一些软件库的支持。如果应用程序编写者将各个线程的数据尽量分开的话，那么其实假共享问题并不会很严重。

Adve等（A Comparison of Entry Consistency and Lazy Release Consistency Implementations）指出，对写入探测和写入收集各有两种方案，分别是（1）将原有的页面做个拷贝，（2）修改编译器来探测这些修改，和（1）用时间戳收集写入的时间和（2）对每次更新使用diff。他发现，如果写少，编译器探测修改更有效，否则拷贝更有效；如果页面是迁移性质的，收集写入时间更有效，因为只要发送最后写入的东西就行，而发送diff则可能要发送多次的对同一个地址的写入。但对一个页面很多人共享的情况，diff就占上风，因为每个时间戳携带的信息很少，反而比一个单条diff多了timestamp信息。也就是讲，如果更新在时间域分散、在空间域集中，timestamp更有效，因为他把多个时间的更新buffer掉并且在最后送出；如果更新在空间域分散、在时间域集中，diff更有效，因为他可以一次传送整个空间的不同。当然这两者可以结合起来，就是lazy check（timestamp）+整个页面diff。最后的结论是，EC其实基本上全面吃亏，因为它同步数目更多，需要更多消息来同步，而它要解决的false sharing问题其实是一个artefact，并且可以有很多解决手段。最后，LRC可能就是足够好的模型了。而且，EC需要编译器支持才能在一些场景中有所表现，而LRC是不需要的。值得惊叹的是，对于LRC，把整个页拿去做Consistency也能和EC按缓存行处理共享打平。更重要的，通过数据预取，LRC的问题能够被进一步掩盖。


1.各种最基本的软硬件设计，在1996年基本就完成演化。
2.有人指出，DSM之所以得不到推广，不是性能问题（LRC一项就可以把性能拉近到10%），主要是缺少支持。


可能的问题：Graph，KV-Stores，BSP，new applications，Easy to Program. ML-training pipelines. Unified programming effort. Blockchain sell. Coherence vs Persistence. 

1.First draft -> make assessment, re-written, reorganized. (from reviewer’s perspective)..
2.

2015年
Xu等（Tardis: Time Traveling Coherence Algorithm for Distributed Shared Memory）提出一种新的硬件一致性缓存协议Tardis。该缓存协议能够避免目录协议潜在的O（MN）空间开销，而只需要O（MlogN）空间开销。它的核心思想是，如果产生了一个新的数据副本，老副本并不需要立即失效或被覆写，只要保证那个副本在老副本失效之后才可见就好了。这有点类似Quiescence。但，Tardis似乎需要一个中心化的TM，这个玩意肯定不是LAN Scalable的，他最多是在数据中心的内存机柜的语境下Scalable。这一点在之前都已经提示过了，但不知是否有办法让这个TM也Distributed。使用的模拟器仍然是Graphite。但这确实给我们一种启示，就是我们可以从那些弱模型的本质出发来推导缓存一致性协议，有些时候可以降低成本。直觉地，越弱的模型要求的缓存一致性协议也越简单。但是，那些弱鸡缓存一致性协议会对程序员或者数据结构提出要求，这不是个好兆头。一个问题是，这个分界线应该被放在哪。Yu等（Tardis 2.0: Optimized Time Traveling Coherence for Relaxed Consistency Models）进一步优化了这个Tardis，他现在支持TSO。


2019年
Pendulum借鉴了Tardis的思想，使用基于安定时间的缓存一致性协议。他将运行关键任务的核与运行非关键任务的核分开了，其具体做法是，允许非关键核的通信抢占非关键核的Cache控制器通信，这样就避免了关键核访问其它非关键核的Exclusive内存时候造成的等待。

\end{document}