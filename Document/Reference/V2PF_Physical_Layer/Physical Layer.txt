Physical Layer
       --RDMA/RoCE
       ①RDMA（Remote Direct Memory Access）
       随着技术飞速发展，业务应用有越来越多的数据需要从网络中获取，对数据中心网络交换速度和性能要求越来越高。传统的TCP/IP通信存在的主要问题即I/O瓶颈问题，在高速网络环境下与网络I/O相关的主机处理的高开销限制了机器之间的传输带宽。具体来说，传统TCO/IP网络通信通过内核发送消息，需要在内核中频繁进行协议封装和解封操作，造成很大的数据移动和复制开销，性能低下；且网络通信协议在内核中处理，很难支持新的网络协议和消息协议以及发送和接受接口，灵活性低下。
       RDMA，全称远程直接内存访问，是为解决网络传输中服务器端数据处理的延迟而产生的，实现了更快更轻量级的网络通信。RDMA将用户应用中的数据直接传入服务器的存储区，通过网络将数据从一个系统快速传输到远程系统的存储器中，消除了传输过程中多次数据复制和文本交换的操作，降低了CPU的负载。
       同TCP/IP对比，RDMA利用栈旁路和零拷贝技术实现了低延迟，同时减少了CPU占用，减少了内存带宽瓶颈，提高了很高的带宽利用率；且提供了基于I/O的通道，允许一个应用程序通过RDMA设备对远程虚拟内存进行直接读写。
       RDMA技术主要包括：
       IB（InfiniBand）：基于InfiniBand架构的RDMA技术，由IBTA（InfiniBand Trade Association）提出。搭建基于IB技术的RDMA网络需要专用的IB网卡和IB交换机。
       iWARP（Internet Wide Area RDMA Protocal）：基于TCP/IP协议的RDMA技术，由IETF标准定义。iWARP支持在标准以太网基础设施上使用RDMA技术，但服务器需要使用支持iWARP的网卡。
       RoCE（RDMA over Converged Ethernet）：基于以太网的RDMA技术，也是由IBTA提出。RoCE支持在标准以太网基础设施上使用RDMA技术，但是需要交换机支持无损以太网传输，需要服务器使用RoCE网卡。
       RDMA技术优势：
       1.零拷贝：在不涉及到网络软件栈的情况下，应用程序能够直接执行数据传输。数据可以直接发送到缓冲区或者直接从缓冲区里接收，无需被复制到网络层；
       2.存在内核旁路，应用程序可以直接在用户态执行数据传输，不需要在内核态与用户态之间做上下文切换；
       3.不需要CPU干预；
       4.RDMA无需操作系统和TCP/IP协议的介入，可以轻易的实现超低延时的数据处理、超高吞吐量传输;
       5.数据被处理为离散消息而不是流，消除了应用程序将流切割为不同消息/事务的需求；
       6.支持分散/聚合条目，即支持读取多个内存缓冲区然后作为一个流发出去或者接收一个流然后写入到多个内存缓冲区去；
       7.提供的所有接口都是异步通信接口，可以更加便利的实现计算和通信分离；
       ②RoCE(RDMA over Converged Ethernet)
       在高性能计算（HPC）系统的发展初期，通常选择定制网络方案来有效解决以太网解决方案的限制，增强带宽、降低延迟、改善拥塞控制。2010年，IBTA推出了RoCE协议技术标准，随后于2014年发布了RoCEv2协议技术标准，大幅提升网络带宽。这种转变打破了以太网在排名前500的HPC集群中使用率下降的趋势，使以太网在排名中保持了重要地位。
       对于RoCE协议，是一种集群网络通信协议，它实现在以太网上进行远程直接内存访问（RDMA）。作为TCP/IP协议的特色功能，该协议将数据包的发射/接收任务转移到网络适配器上，改变了系统进入内核模式的需求。
       RoCE协议包括两个版本：
       1.RoCE v1：2010年4月，IBTA发布了RoCE，作为链路层协议运行，要求通信双方在相同的第2层网络中，用IB网络层代替了TCP/IP网络层，故不支持IP路由功能；在RoCE数据包的链路层数据帧中，以太网类型字段的值由IEEE指定为0x8915，明确标识其为RoCE数据包；由于其数据帧不带IP头部，所以只能在L2子网内通信；
       2.RoCE v2：该协议是在RoCE协议的基础上持续优化，作为网络层协议运行，使该协议数据包可以在第3层进行路由，提供了更好的可扩展性；RoCEv2通过融合以太网网络层和使用UDP协议的传输层，改造了RoCE协议所使用的InfiniBand（IB）网络层。它利用以太网网络层中IP数据报的DSCP和ECN字段来实现拥塞控制。这使得RoCE v2协议数据包可以进行路由，确保了更好的可扩展性。
       Soft-RoCE:在高性能以太网网卡领域，在特定情况下某些网卡不支持RoCE，为了填补这一空白，IBIV、迈络思和其品牌的合作，催生了开源项目Soft-RoCE。与RoCE不同的是，Soft-RoCE适用于任何以太环境，无需依赖NIC、switch、12Qos等支持。这个项目适用于设备不支持RoCE协议的节点，使它们能够与设备RoCE支持的节点一起使用Soft-RoCE进行通信，充分发挥其性能优势。尤其是在数据中心等应用场景中，将升级限制在具有RoCE支持的以太网卡的高I/O存储服务器上，可以显著提高整体性能和可扩展性。此外RoCE和Soft-RoCE的组合适应了逐步集群升级的需求，避免了同时进行全面升级的必要性。
       RoCE优点：
       1.减少与复制、封装和解封相关的开销；
       2.很大程度上降低了以太网通信的延迟；
       3.通信过程中充分利用CPU资源；
       4.提供了增强的部署灵活性；
       5.采用双重拥塞控制机制，减轻了网络拥塞，进行更可靠的数据传输，提高了带宽的有效利用率；
       RoCE缺点：
       1.对于在动态流量模式下保持低延迟，由于关注重心转移到拥塞控制上，高度动态的HPC流量模式的复杂性对RoCE构成了挑战，导致在这方面性能不佳；
       2.由于以太网底层结构的复杂性，将RoCE应用于HPC存在困难；
       3.拥塞控制中优先级流量控制（PFC）依赖于暂停控制帧来防止接收过多数据包，容易导致数据包丢失；数据中心量化拥塞通知（DCQCN）遵循一组固定的减速和加速策略公式，同InfiniBand的自定义策略相比，灵活性较低；还有优化空间；
https://support.huawei.com/enterprise/zh/doc/EDOC1100203347https://www.h3c.com/cn/Service/Document_Software/Document_Center/Home/Switches/00-Public/Learn_Technologies/White_Paper/RDMA_Tech_White_Paper-6W100/?CHID=365285
https://www.h3c.com/cn/Service/Document_Software/Document_Center/Home/Switches/00-
https://blog.csdn.net/qq_40323844/article/details/90680159
https://community.fs.com/cn/article/roce-technology-in-high-performance-computing.html
https://www.jianshu.com/p/85e7309c6187
       --CXL/PCle
       PCle和CXL都是计算机领域中使用的高速串行总线技术。       
       PCL是英特尔1991年推出的一种用于定义局部总线的标准，工作频率在33MHz，传输带宽为132MB/s（32位），以满足当时处理器的应用需要。
       1993年，64位的PCL-X问世，以应对服务器产品的需求。
       2001年，英特尔公布了第三代I/O技术——3GIO技术，即PCLe，以高性能、高扩展性、高可靠性以及出色的兼容性取代了包括AGP和PCL在内的所有内部总线并不断升级，每一代的发展，最明显的特征是速率翻倍。
       2019年5月底公布的PCle5.0，其以32Gb/s的单通道带宽与32GT/s（Giga Transmission per second）每通道数据传输速率，满足了当时绝大多数的需求。
       2022年推出的PCLe6.0，实现了64GT/s的传输速率，进一步推动了技术发展。
       PCLe7.0是下一代计算机互联技术，旨在将每个引脚的数据传输速度提高至128GT/s，这种速度对于未来的数据中心以及需要更快数据传输速率的人工智能和高性能计算应用来说非常方便。
       PCLe是用于连接计算机内部组件的一种标准接口技术，为兼容之前的PCL总线设备，虽作为一种高速串行点对点双通道高带宽传输总线，无法在物理层上兼容PCL总线，但在软件层上面却可以兼容PCL总线，且采用的全双工传输设计，并采用差分对进行收发，以提高总线的性能。和PCL总线相比，PCLe可以根据应用的需要来调整PCLe设备的带宽。由于服务器有巨大的内存池和数量庞大的基于PCLe运算加速器，且每个上面都有很大的内存，而这种内存分割造成了巨大的浪费、不便和性能下降，且虽然PCLe已经有了很多改进，但其难以满足现代计算机处理器和加速器之间的高带宽、低延迟的通信需求，因此，CXL技术应运而生。
       CXL技术是一种新型的高速互联技术，它允许在计算机系统内部的不同组件之间进行快速、可靠的数据传输，还支持内存共享和虚拟化，使设备之间的协作更加紧密和高效。最初在2020年由英特尔、AMD和其他公司联合推出，并得到了包括谷歌、微软等公司在内的大量支持，旨在提供更高的数据吞吐量和更低的延迟，以满足现代计算和存储系统的需求。
       CXL协议包含三个子协议：
       1.CXL.io:通过PCLe总线连接CPU和外部设备，是CXL规范中定义的物理层窗口，这样CPU就可以与外部设备共享内存，并且可以直接访问外部设备的I/O资源，从而可以提供比传统PCLe更低的延迟、更高的带宽和更好的可扩展性；
       2.CXL.cache:这种模式可以通过将内存缓存到外部设备中来提高性能，该模式允许CPU在本地缓存中保留最常用的数据，而将不常用的数据保存在外部设备中，从而可以减少内存访问的时间，提高整体系统的性能；
       3.CXL.memory:该模式下可以将外部设备作为主内存使用，从而可以实现更大的内存容量。CXL.memory模式允许CPU将外部设备看作是扩展内存，从而可以存储更多数据，还可以提高系统的可靠性，因为即使内存发生了故障，CPU仍可以通过外部设备继续运行。
       CXL技术优势：
       支持高带宽、低延迟数据传输、具有更好的灵活性和可扩展性、可以实现混合使用不同类型的硬件设备，面向不同处理器架构；
       CXL技术实现面临的挑战：
       1.CXL技术的实现需要高度复杂的系统设计和集成，这意味着需要适应不同的硬件、软件和工具，可以通过提供转换器和中间件，以支持现有系统的升级和兼容性；
       2.由于CXL技术涉及到底层硬件操作，并且可能涉及多个设备之间的数据共享，因此安全风险是一个重要考虑因素，需要采取适当的措施确保数据安全和隐私，例如访问控制、认证和加密；
       3.CXL技术需要提供高速数据传输和低延迟，以满足对计算能力和存储能力的要求，这需要高效的协议和优化的硬件和软件设计，比如增加硬件加速、内存缓存和错误纠正功能；
       4.CXL技术需要与现有的接口和协议兼容，以支持旧设备和系统的升级，这需要适当的转换器和中间件；可以通过指定统一的标准和规范来确保不同厂商的设备和系统之间的兼容性和互操作性；
       同PCLe技术对比：
       1.CXL的带宽比PCIe高得多，CXL 2.0标准最高可以达到32 GT/s，而PCIe 5.0的带宽只能达到16 GT/s；
       2.CXL和PCIe都具有低延迟的特点，但CXL在延迟方面稍微优于PCIe；
       3.CXL支持内存扩展、缓存一致性和设备直接内存访问等功能，而这些功能PCLe并不具备；
       4.PCIe主要用于连接外部设备，如GPU、网卡和存储设备，而CXL则更加灵活，可用于连接处理器、存储设备、网络适配器和其他外围设备，适用范围更广；
       5.由于CXL是相对较新的技术，许多旧设备可能无法与其兼容，PCIe则已经成为了一种通用的连接标准，并且得到广泛应用；
       6.目前来看，CXL硬件和设备的成本相对较高，而PCIe则更加普及和经济实惠；
https://blog.csdn.net/zhuzongpeng/article/details/137807939
https://mp.weixin.qq.com/s?__biz=MzIwNTUxNDgwNg==&mid=2247490360&idx=1&sn=a68beba8e16ca49cf3e1933574cbfb98&chksm=972eea61a0596377fde85a1f8dd184b0fe4299ed8462523a2b9cf31f223b0036a12571170b79&scene=21#wechat_redirect
https://blog.csdn.net/kunkliu/article/details/93993436
https://blog.csdn.net/Long_xu/article/details/131317471
https://blog.csdn.net/HackEle/article/details/135578883
       --Infiniband
       InfiniBand（无限带宽，IB）是一种高性能计算和数据中心网络架构，其设计目标是通过提供低延迟、高带宽以及可扩展性来满足大规模计算和数据传输的需求。
       ①Infiniband发展历程：
       现代意义上的数字计算机，从诞生之日起，一直采用的为冯·诺依曼架构；
       上世纪90年代早期，为了支持越来越多的外部设备，英特尔公司率先在标准PC架构中引入PCL（外部部件互连标准）总线设计。
       不久后，互联网进入高速发展阶段。线上业务和用户规模的不断增加，给IT系统的承载能力带来了很大的挑战；当时，在摩尔定律的加持下，CPU、内存、硬盘等部件都在快速升级。而PCI总线，升级速度缓慢，大大限制了I/O性能，成为整个系统的瓶颈。
       为了解决这个问题，英特尔、微软、SUN公司主导开发了“Next Generation I/O（NGIO）”技术标准。而IBM、康柏以及惠普公司，则主导开发的“Future I/O（FIO）”。IBM这三家公司，还合力搞出了PCI-X标准（1998年）。
       1999年，FIO Developers Forum和NGIO Forum进行了合并，创立了InfiniBand贸易协会（InfiniBand Trade Association，IBTA）。
       2000年，InfiniBand架构规范1.0版本正式发布，来取代PCL总线。它引入了RDMA协议，具有更低的延迟，更大的带宽，更高的可靠性，可以实现更强大的I/O性能。随着高性能计算需求不断增长，InfiniBand技术继续高歌猛进，市场份额不断提升，并于2015年首次实现了对以太网技术的逆袭。
       2010年4月，IBTA发布了RoCE，将InfiniBand中的RDMA技术“移植”到以太网，2014年又提出了更加成熟的RoCE v2，大幅缩小了和InfiniBand之间的技术性能差距，结合本身固有的成本和兼容性优势，又开始反杀回来。
       随着AIGC大模型崛起，整个社会对高性能计算和智能计算的需求井喷。而英伟达公司成功收购了迈络思，将自家的GPU算力优势与Mellanox的网络优势相结合，就等于打造了一个强大的“算力引擎”。在算力基础设施上，英伟达毫无疑问占据了领先优势。
       如今，在高性能网络的竞争上，就是InfiniBand和高速以太网的缠斗。双方势均力敌。不差钱的厂商，更多会选择InfiniBand。而追求性价比的，则会更倾向高速以太网。
       ②InfiniBand架构：
       类似于传统的TCP/IP模型，InfiniBand在分布式存储领域，尤其是在分布式并行计算（DPC）场景的存储前端网络中得到广泛应用。其强调高性能、低延迟，适用于大规模并行计算机集群和需要高吞吐量的应用场景。
       TCP/IP则更为普遍地应用于商业网络和一般互联网通信。它是目前互联网通信的主流协议，用于支持各种应用，包括网页浏览、电子邮件、文件传输等。
       1.物理层：InfiniBand采用多种传输介质和硬件规格，包括高速电缆、光纤和连接器，以支持高带宽、低延迟的数据传输；
       2.链路层：本地标识符（LID）和转发表的使用为数据包的定位和传输提供了关键支持；
       3.网络层：子网划分和子网管理器的角色确保了网络的灵活性和高效性；
       4.传输层：引入了远程直接内存访问（RDMA）技术，通过硬件级别的数据传输实现了低延迟和高吞吐量；
       5.应用层：主要应用于HPC高性能计算、大型数据中心、高性能存储等场景中；
       ③Infiniband组成单元主要分为四类：
       （1）HCA（Host Channel Adapter），它是连接内存控制器和TCA的桥梁；
       （2）TCA(Target Channel Adapter)，它将I/O设备（例如网卡、SCSI控制器）的数字信号打包发送给HCA；
       （3）Infiniband link，它是连接HCA和TCA的光纤，InfiniBand架构允许硬件厂家以1条、4条、12条光纤3种方式连结TCA和HCA；
       （4）交换机和路由器：无论是HCA还是TCA，其实质都是一个主机适配器，它是一个具备一定保护功能的可编程DMA（Direct Memory Access，直接内存存取 ）引擎；
       ④InfiniBand技术优势：
       1.InfiniBand最突出的优势，是率先引入RDMA（远程直接数据存取）协议。
              -传统模式 VS RDMA模式
              --RDMA相当于是一个“消灭中间商”的技术。
              --RDMA的内核旁路机制，允许应用与网卡之间的直接数据读写，将服务器内的数据传输时延降低到接近1us。
              --同时，RDMA的内存零拷贝机制，允许接收端直接从发送端的内存读取数据，绕开了核心内存的参与，极大地减少了CPU的负担，提升CPU的效率。
       2.InfiniBand支持多种拓扑结构，包括点对点、树状、环形和网状，使它适用于不同规模的计算机集群；
       3.InfiniBand网络采用交换式网络拓扑，其中交换机负责路由数据包，有助于减少网络拥塞、提高性能；
       4.InfiniBand网络允许在用户空间中实现网络堆栈，使得应用程序能够直接处理网络协议。这样一来，应用程序不再需要通过操作系统的内核空间进行数据传输，而是可以直接在用户空间中完成网络操作，提高了效率和灵活性；
       5.InfiniBand还支持零拷贝技术，通过这一技术，应用程序可以直接在内存中操纵数据，而无需将数据复制到中间缓冲区。这降低了数据传输的开销，提高了效率；
       6.InfiniBand可以在虚拟化环境中使用，支持虚拟机之间的高性能通信；
       7.InfiniBand网络可以划分为多个子网，每个子网都有自己的拓扑结构和管理策略；
       ⑤InifiniBand缺点：
       1.InfiniBand设备和基础设施的成本通常较高，可能限制了一些小型系的采用；
       2.尽管InfiniBand在超算领域有一定份额，但在通用数据中心中市场份额相对较小；
       3.InfiniBand适用于高性能计算、大规模数据处理和存储系统，但对于一般企业来说，以太网可能更常见，InfiniBand不适用于所有应用；
https://baike.baidu.com/item/Infiniband/1963979
https://cloud.tencent.com/developer/article/2314766
https://developer.aliyun.com/article/1444457
https://cloud.tencent.com/developer/news/384580
https://blog.csdn.net/ADOP_BitRate/article/details/138182997#:~:text=%F0%9F%94%8A%E5%AD%90%E7%BD%91%E7%AE%A1%E7%90%86%EF%BC%9A%20InfiniBand%E7%BD%91%E7%BB%9C%E5%8F%AF%E4%BB%A5%E5%88%92%E5%88%86%E4%B8%BA%E5%A4%9A%E4%B8%AA%E5%AD%90%E7%BD%91%EF%BC%8C%E6%AF%8F%E4%B8%AA%E5%AD%90%E7%BD%91%E5%8F%AF%E4%BB%A5%E6%9C%89%E8%87%AA%E5%B7%B1%E7%9A%84%E6%8B%93%E6%89%91%E7%BB%93%E6%9E%84%E5%92%8C%E7%AE%A1%E7%90%86%E7%AD%96%E7%95%A5%E3%80%82%20%E7%BC%BA%E7%82%B9%EF%BC%9A%20%F0%9F%94%8A%E6%88%90%E6%9C%AC%E8%BE%83%E9%AB%98%EF%BC%9A,InfiniBand%E8%AE%BE%E5%A4%87%E5%92%8C%E5%9F%BA%E7%A1%80%E8%AE%BE%E6%96%BD%E7%9A%84%E6%88%90%E6%9C%AC%E9%80%9A%E5%B8%B8%E8%BE%83%E9%AB%98%EF%BC%8C%E8%BF%99%E5%8F%AF%E8%83%BD%E9%99%90%E5%88%B6%E4%BA%86%E4%B8%80%E4%BA%9B%E5%B0%8F%E5%9E%8B%E7%B3%BB%E7%BB%9F%E7%9A%84%E9%87%87%E7%94%A8%E3%80%82%20%F0%9F%94%8A%E5%B8%82%E5%9C%BA%E4%BB%BD%E9%A2%9D%E8%BE%83%E5%B0%8F%EF%BC%9A%20%E5%B0%BD%E7%AE%A1%E5%9C%A8%20%E8%B6%85%E7%BA%A7%E8%AE%A1%E7%AE%97%E6%9C%BA%20%E9%A2%86%E5%9F%9F%E6%9C%89%E4%B8%80%E5%AE%9A%E4%BB%BD%E9%A2%9D%EF%BC%8C%E4%BD%86InfiniBand%E5%9C%A8%E9%80%9A%E7%94%A8%E6%95%B0%E6%8D%AE%E4%B8%AD%E5%BF%83%E4%B8%AD%E7%9A%84%E5%B8%82%E5%9C%BA%E4%BB%BD%E9%A2%9D%E7%9B%B8%E5%AF%B9%E8%BE%83%E5%B0%8F%E3%80%82
       --OmniPath:
       回顾IBTA的9个主要董事成员，只有Mellanox和Emulex专门在做InfiniBand，其他成员只是扮演了InfiniBand的角色。但Emulex由于业务不景气在2015年2月被Avago收购，Ologic的InfiniBand业务也在2012年全部卖给了Intel。收购后的Intel又另辟蹊径，推出了自己的一整套叫做“True Scale Fabric”的高性能计算架构，独立提出了一套OmniPath Host Fabric Interface新接口和对应交换机产品，每个端口支持100G速率，直接叫板InfiniBand EDR。
       OmniPath是Intel公司开发的一种高性能、低延迟的硬件和软件架构，其信号传输性能可应对未来的高性能计算工作量，与当今的网络架构相比，其性能更佳且成本更低。
       ①OmniPath网络架构：
       第1层-物理层：利用现有的以太网和InfiniBand PHY标准；
       第1.5层-链路传输协议：提供可靠的第2层数据包传输、流量控制和单链路链路控制；
       第2层-数据链路层：提供结构寻址、交换、资源分配和分区支持；
       第4-7层-传输层到应用层：提供软件库和HFI之间的接口，利用开放结构作为基本的软件基础结构；
       ②Omni-Path主要有以下3部分组件：
       （1）HFI – Host Fabric Interface 提供主机，服务和管理节点的光纤连接
       （2）Switches 提供大规模的节点之间的任意拓扑连接
       （3）Fabric Manager 提供集中化的对光纤资源的provisioning 和监控
       ③相比InfiniBand ，Intel Omni-Path Architecture 架构设计目标特性：
       1. 通过CPU/Fabric integration 来提高cost, power, and density
       2. Host主机端的优化实现来高速的MPI消息，低延迟的高扩展性的架构
       3. Enhanced Fabric Architecture 来提供超低的端到端延迟，高效的纠错和增强的QoS，并且超高的扩展性
       ④How it died：
       1.Omni-Path具备低延迟、介质层和网络层优化、CPU集成支持、生态广泛等特点，但是对比InfiniBand并没有跨数量级的优势，带宽目前只有100Gbps，规划中的下一代也才200Gbps，Infiniband HDR则已经实现400Gbps。
       2.从现有市场看，OmniPath主要在科研领域高校得到了追捧，可以满足小规模集群，部分场景的应用需求，而组织更大规模集群，则需要技术专注和长期积累；
       3.对于英特尔来说，更加看重生态圈的建设，如果是作为OmniPath网络厂商，策略就变得有些奇怪，涉足网络设备显得有些动作变形；
       4.英特尔新总裁Robert Swan，CFO出身，职业特点决定了他没有更多技术情节，财务表现才是专业；
       ⑤ScaleMP vSMP容错解决策略:
       1.硬件冗余：
       （1）服务器级别的冗余：使用具有冗余电源、冗余网络接口和其他关键组件的服务器，减少单点故障的影响；
       （2）网络互连的冗余：采用InfiniBand、OmniPath等高性能互连技术，确保在网络拓扑中设置冗余路径，以防止单个网络链路或设备故障影响整个系统；
       2.高可用性配置：
       （1）使用虚拟化平台或容器技术，结合高可用性集群软件，在节点故障时实现自动迁移和恢复虚拟机或容器；
       （2）在ScaleMP vSMP配置中，可以将不同物理节点上的虚拟内存区域映射到不同的硬件资源，增加故障隔离性和容错能力；
       3.配置和使用监控工具实时监视硬件和软件运行状态；
       4.定期对数据进行备份，并确保备份数据可靠和安全存储；
https://news.eeworld.com.cn/mp/Amphenol_ICC/a62418.jspx
https://www.cnblogs.com/allcloud/p/8945544.html
https://www.sohu.com/a/151435033_632967
http://www.dostor.com/p/59937.html
https://news.mydrivers.com/1/651/651469.htm